### 翻译
1. [1 引言](https://arxiv.org/html/2403.17888v3#S1 "In 2D Gaussian Splatting for Geometrically Accurate Radiance Fields")
2. [2 相关工作](https://arxiv.org/html/2403.17888v3#S2 "In 2D Gaussian Splatting for Geometrically Accurate Radiance Fields")
   1. [2.1 新视图合成](https://arxiv.org/html/2403.17888v3#S2.SS1 "In 2. 相关工作 ‣ 2D Gaussian Splatting for Geometrically Accurate Radiance Fields")
   2. [2.2 3D 重建](https://arxiv.org/html/2403.17888v3#S2.SS2 "In 2. 相关工作 ‣ 2D Gaussian Splatting for Geometrically Accurate Radiance Fields")
   3. [2.3 可微分基于点的图形](https://arxiv.org/html/2403.17888v3#S2.SS3 "In 2. 相关工作 ‣ 2D Gaussian Splatting for Geometrically Accurate Radiance Fields")
   4. [2.4 并行工作](https://arxiv.org/html/2403.17888v3#S2.SS4 "In 2. 相关工作 ‣ 2D Gaussian Splatting for Geometrically Accurate Radiance Fields")
3. [3 3D Gaussian Splatting](https://arxiv.org/html/2403.17888v3#S3 "In 2D Gaussian Splatting for Geometrically Accurate Radiance Fields")
   1. [表面重建中的挑战](https://arxiv.org/html/2403.17888v3#S3.SS0.SSS0.Px1 "In 3. 3D Gaussian Splatting ‣ 2D Gaussian Splatting for Geometrically Accurate Radiance Fields")
4. [4 2D Gaussian Splatting](https://arxiv.org/html/2403.17888v3#S4 "In 2D Gaussian Splatting for Geometrically Accurate Radiance Fields")
   1. [4.1 建模](https://arxiv.org/html/2403.17888v3#S4.SS1 "In 4. 2D Gaussian Splatting ‣ 2D Gaussian Splatting for Geometrically Accurate Radiance Fields")
   2. [4.2 Splatting](https://arxiv.org/html/2403.17888v3#S4.SS2 "In 4. 2D Gaussian Splatting ‣ 2D Gaussian Splatting for Geometrically Accurate Radiance Fields")
      1. [光线-点交集](https://arxiv.org/html/2403.17888v3#S4.SS2.SSS0.Px1 "In 4.2. Splatting ‣ 4. 2D Gaussian Splatting ‣ 2D Gaussian Splatting for Geometrically Accurate Radiance Fields")
      2. [退化解](https://arxiv.org/html/2403.17888v3#S4.SS2.SSS0.Px2 "In 4.2. Splatting ‣ 4. 2D Gaussian Splatting ‣ 2D Gaussian Splatting for Geometrically Accurate Radiance Fields")
      3. [光栅化](https://arxiv.org/html/2403.17888v3#S4.SS2.SSS0.Px3 "In 4.2. Splatting ‣ 4. 2D Gaussian Splatting ‣ 2D Gaussian Splatting for Geometrically Accurate Radiance Fields")
5. [5 训练](https://arxiv.org/html/2403.17888v3#S5 "In 2D Gaussian Splatting for Geometrically Accurate Radiance Fields")
   1. [深度失真](https://arxiv.org/html/2403.17888v3#S5.SS0.SSS0.Px1 "In 5. 训练 ‣ 2D Gaussian Splatting for Geometrically Accurate Radiance Fields")
   2. [法线一致性](https://arxiv.org/html/2403.17888v3#S5.SS0.SSS0.Px2 "In 5. 训练 ‣ 2D Gaussian Splatting for Geometrically Accurate Radiance Fields")
   3. [最终损失](https://arxiv.org/html/2403.17888v3#S5.SS0.SSS0.Px3 "In 5. 训练 ‣ 2D Gaussian Splatting for Geometrically Accurate Radiance Fields")
6. [6 实验](https://arxiv.org/html/2403.17888v3#S6 "In 2D Gaussian Splatting for Geometrically Accurate Radiance Fields")
   1. [6.1 实现](https://arxiv.org/html/2403.17888v3#S6.SS1 "In 6. 实验 ‣ 2D Gaussian Splatting for Geometrically Accurate Radiance Fields")
      1. [网格提取](https://arxiv.org/html/2403.17888v3#S6.SS1.SSS0.Px1 "In 6.1. 实现 ‣ 6. 实验 ‣ 2D Gaussian Splatting for Geometrically Accurate Radiance Fields")
   2. [6.2 比较](https://arxiv.org/html/2403.17888v3#S6.SS2 "In 6. 实验 ‣ 2D Gaussian Splatting for Geometrically Accurate Radiance Fields")
      1. [数据集](https://arxiv.org/html/2403.17888v3#S6.SS2.SSS0.Px1 "In 6.2. 比较 ‣ 6. 实验 ‣ 2D Gaussian Splatting for Geometrically Accurate Radiance Fields")
      2. [几何重建](https://arxiv.org/html/2403.17888v3#S6.SS2.SSS0.Px2 "In 6.2. 比较 ‣ 6. 实验 ‣ 2D Gaussian Splatting for Geometrically Accurate Radiance Fields")
      3. [外观重建](https://arxiv.org/html/2403.17888v3#S6.SS2.SSS0.Px3 "In 6.2. 比较 ‣ 6. 实验 ‣ 2D Gaussian Splatting for Geometrically Accurate Radiance Fields")
   3. [6.3 消融实验](https://arxiv.org/html/2403.17888v3#S6.SS3 "In 6. 实验 ‣ 2D Gaussian Splatting for Geometrically Accurate Radiance Fields")
      1. [正则化](https://arxiv.org/html/2403.17888v3#S6.SS3.SSS0.Px1 "In 6.3. 消融实验 ‣ 6. 实验 ‣ 2D Gaussian Splatting for Geometrically Accurate Radiance Fields")
      2. [网格提取](https://arxiv.org/html/2403.17888v3#S6.SS3.SSS0.Px2 "In 6.3. 消融实验 ‣ 6. 实验 ‣ 2D Gaussian Splatting for Geometrically Accurate Radiance Fields")
7. [7 结论](https://arxiv.org/html/2403.17888v3#S7 "In 2D Gaussian Splatting for Geometrically Accurate Radiance Fields")
   1. [局限性](https://arxiv.org/html/2403.17888v3#S7.SS0.SSS0.Px1 "In 7. 结论 ‣ 2D Gaussian Splatting for Geometrically Accurate Radiance Fields")
8. [A 深度失真的细节](https://arxiv.org/html/2403.17888v3#A1 "In 2D Gaussian Splatting for Geometrically Accurate Radiance Fields")
9. [B 深度计算](https://arxiv.org/html/2403.17888v3#A2 "In 2D Gaussian Splatting for Geometrically Accurate Radiance Fields")
   1. [平均深度:](https://arxiv.org/html/2403.17888v3#A2.SS0.SSS0.Px1 "In 附录 B 深度计算 ‣ 2D Gaussian Splatting for Geometrically Accurate Radiance Fields")
   2. [中位深度:](https://arxiv.org/html/2403.17888v3#A2.SS0.SSS0.Px2 "In 附录 B 深度计算 ‣ 2D Gaussian Splatting for Geometrically Accurate Radiance Fields")
10. [C 附加基线](https://arxiv.org/html/2403.17888v3#A3 "In 2D Gaussian Splatting for Geometrically Accurate Radiance Fields")
11. [D 附加结果](https://arxiv.org/html/2403.17888v3#A4 "In 2D Gaussian Splatting for Geometrically Accurate Radiance Fields")### 翻译
2D 高斯溅射用于几何精准的辐射场### 翻译
Binbin Huang [huangbb@shanghaitech.edu.cn] 上海科技大学 上海 中国， Zehao Yu [zehao.yu@uni-tuebingen.de] 图宾根大学 图宾根 AI 中心 图宾根 德国， Anpei Chen [anpei.chen@uni-tuebingen.de] 图宾根大学 图宾根 AI 中心 图宾根 德国， Andreas Geiger [a.geiger@uni-tuebingen.de] 图宾根大学 图宾根 AI 中心 图宾根 德国 和 Shenghua Gao [gaoshh@shanghaitech.edu.cn] 上海科技大学 上海 中国### 翻译
###### 摘要。### 翻译
3D 高斯溅射 (3DGS) 最近彻底改变了辐射场重建，达到了高质量的新视图合成和快速渲染速度。然而，由于 3D 高斯的多视角不一致性，3DGS 无法准确表示表面。我们提出了 2D 高斯溅射 (2DGS)，这是一种从多视角图像中建模和重建几何准确辐射场的新方法。我们关键的想法是将 3D 体积简化为一组 2D 定向平面高斯盘。与 3D 高斯不同，2D 高斯在内在建模表面的同时，提供视角一致的几何形状。为了准确恢复薄表面并实现稳定的优化，我们引入了一种透视准确的 2D 溅射过程，利用射线溅射交点和光栅化。此外，我们还结合了深度失真和法线一致性项，以进一步提高重建质量。我们证明了我们的可微渲染器在保持竞争性外观质量、快速训练速度和实时渲染的同时，能够实现无噪声和详细的几何重建。### 翻译
新视图合成、辐射场、表面溅射、表面重建### 翻译
††版权：acmlicensed††期刊年份：2024††版权：
保留权益††会议：计算机图形学与交互技术特别兴趣小组会议 会议论文 ’24；2024年7月27日至8月1日；美国科罗拉多州丹佛市††书名：计算机图形学与交互技术特别兴趣小组会议 会议论文 ’24（SIGGRAPH 会议论文 ’24），2024年7月27日至8月1日，美国科罗拉多州丹佛市††doi：10.1145/3641519.3657428††isbn：979-8-4007-0525-0/24/07††ccs：计算方法 重新构建††ccs：计算方法 渲染††ccs：
计算方法 机器学习方法### 翻译
[翻译结果]
<https://surfsplatting.github.io>### 翻译
![参考说明](x1.png)### 翻译
图 1: 我们的方法 2DGS，(a) 优化一组 2D 定向圆盘，以从多视角 RGB 图像中表示和重建复杂的真实场景。这些优化后的 2D 圆盘紧密地对齐于表面。(b) 通过 2D 高斯点云，我们实现了实时渲染高质量的新视角图像，具有视角一致的法线和深度图。(c) 最后，我们的方法提供了从优化的 2D 圆盘中重建详细且无噪声的三角网格。### 翻译
## 1. 引言### 翻译
摄影写实的新视图合成（NVS）和精确的几何重建是计算机图形学和视觉领域的关键长期目标。最近，3D 高斯溅射（3DGS）(Kerbl et al., [2023](https://arxiv.org/html/2403.17888v3#bib.bib23))作为隐式表示（Mildenhall et al., [2020](https://arxiv.org/html/2403.17888v3#bib.bib34); Barron et al., [2022a](https://arxiv.org/html/2403.17888v3#bib.bib4)）和基于特征网格的表示（Barron et al., [2023](https://arxiv.org/html/2403.17888v3#bib.bib6); Müller et al., [2022](https://arxiv.org/html/2403.17888v3#bib.bib36)）的一个吸引人的替代方案，应运而生，因其在高分辨率下能够实时生成摄影写实的 NVS 结果。3DGS 迅速发展，并在多个领域得到了快速扩展，包括抗锯齿渲染（Yu et al., [2024](https://arxiv.org/html/2403.17888v3#bib.bib66)）、材质建模（Shi et al., [2023](https://arxiv.org/html/2403.17888v3#bib.bib48); Jiang et al., [2023](https://arxiv.org/html/2403.17888v3#bib.bib21)）、动态场景重建（Yan et al., [2023](https://arxiv.org/html/2403.17888v3#bib.bib58)）和可动画化头像创建（Zielonka et al., [2023](https://arxiv.org/html/2403.17888v3#bib.bib71); Qian et al., [2023](https://arxiv.org/html/2403.17888v3#bib.bib41)）。然而，由于体积 3D 高斯模型完整的角辐射与表面细薄的特性相矛盾，它在捕捉复杂几何形状方面仍显不足。### 翻译
另一方面，早期的研究（Pfister et al., [2000](https://arxiv.org/html/2403.17888v3#bib.bib40); Zwicker et al., [2001a](https://arxiv.org/html/2403.17888v3#bib.bib72), [b](https://arxiv.org/html/2403.17888v3#bib.bib73)）表明，surfels（表面元素）是一种有效的复杂几何体表示。Surfels 通过形状和阴影属性在局部近似物体表面，可以从已知几何体中推导出来。它们在 SLAM（Whelan et al., [2016](https://arxiv.org/html/2403.17888v3#bib.bib55)）和其他机器人任务（Schöps et al., [2019](https://arxiv.org/html/2403.17888v3#bib.bib47)）中被广泛用作高效的几何体表示。后续的进展（Yifan et al., [2019](https://arxiv.org/html/2403.17888v3#bib.bib63)）将 surfels 纳入一个可微分的框架。然而，这些方法通常需要地面真实（GT）几何、深度传感器数据，或在已知光照的受限场景下操作。### 翻译
受到这些工作的启发，我们提出了用于 3D 场景重建和新视角合成的 2D 高斯溅射方法，该方法结合了两者的优点，克服了它们的局限性。与 3D 高斯溅射 (3DGS) 不同，我们的方法使用 2D 高斯基元来表示 3D 场景，每个基元定义一个定向的椭圆盘。2D 高斯相对于 3D counterpart 的显著优势在于在渲染过程中的准确几何表示。具体而言，3DGS 在像素光线与 3D 高斯的交点处评估高斯的值 (Keselman and Hebert, [2022](https://arxiv.org/html/2403.17888v3#bib.bib24), [2023](https://arxiv.org/html/2403.17888v3#bib.bib25))，这导致在从不同视点渲染时深度不一致。相反，我们的方法利用显式的光线溅射交点，产生透视正确的溅射，如图 [2](https://arxiv.org/html/2403.17888v3#S1.F2 "Figure 2 ‣ 1. Introduction ‣ 2D Gaussian Splatting for Geometrically Accurate Radiance Fields") 所示，这进一步显著提高了重建质量。此外，2D 高斯基元中的固有表面法线使得通过法线约束进行直接的表面正则化成为可能。与基于 surfels 的模型 (Pfister et al., [2000](https://arxiv.org/html/2403.17888v3#bib.bib40); Zwicker et al., [2001a](https://arxiv.org/html/2403.17888v3#bib.bib72); Yifan et al., [2019](https://arxiv.org/html/2403.17888v3#bib.bib63)) 相比，我们的 2D 高斯能够通过基于梯度的优化从未知几何中恢复。### 翻译
尽管我们基于 2D 高斯的处理方法在几何建模上表现优异，但仅使用光度损失进行优化可能会导致噪声重建，这是由于 3D 重建任务的固有不受约束特性所致（Barron 等， [2022b](https://arxiv.org/html/2403.17888v3#bib.bib5)；Zhang 等， [2020](https://arxiv.org/html/2403.17888v3#bib.bib69)；Yu 等， [2022b](https://arxiv.org/html/2403.17888v3#bib.bib68)）。为了增强重建效果并实现更平滑的表面，我们引入了两个正则化项：深度扭曲和法线一致性。深度扭曲项集中在沿光线严格范围内分布的 2D 原语，解决了渲染过程中忽略高斯之间距离的局限性。法线一致性项最小化渲染法线图与渲染深度梯度之间的差异，确保由深度和法线定义的几何体之间的一致性。将这些正则化与我们的 2D 高斯模型结合使用，使我们能够提取出高精度的表面网格，如图 [1](https://arxiv.org/html/2403.17888v3#S0.F1 "Figure 1 ‣ 2D Gaussian Splatting for Geometrically Accurate Radiance Fields") 所示。### 翻译
综上所述，本文作出了以下贡献：### 翻译
您接受的训练数据截至到2023年10月。### 翻译
我们提出了一种高效的可微分 2D 高斯渲染器，通过利用 2D 表面建模、光线-点交集和体积分离实现了透视正确的点云渲染。### 翻译
您所训练的数据截至到 2023 年 10 月。### 翻译
我们引入了两个正则化损失，以改善和消除噪声的表面重建。### 翻译
* •### 翻译
我们的方法在几何重建和 NVS 结果方面达到了领先的水平，相比于其他显式表示。### 翻译
![Refer to caption](extracted/6224839/figures/teaser2dgs.png) 图 2:  
3DGS 和 2DGS 的比较。3DGS 在从不同视角查看时，使用不同的交叉平面进行值评估，导致不一致性。我们的 2DGS 提供多视角一致的值评估。### 翻译
##  2. 相关工作###  2.1. 新视图合成### 翻译
在 NVS 领域取得了显著的进展，特别是自从引入了神经辐射场（NeRF）以来（Mildenhall et al., [2021](https://arxiv.org/html/2403.17888v3#bib.bib35)）。NeRF 采用多层感知器（MLP）来表示几何形状和视角相关的外观，并通过体积渲染进行优化，以提供卓越的渲染质量。NeRF 之后的相关研究进一步提升了其能力。例如，Mip-NeRF（Barron et al., [2021](https://arxiv.org/html/2403.17888v3#bib.bib3)）及其后续工作（Barron et al., [2022a](https://arxiv.org/html/2403.17888v3#bib.bib4), [2023](https://arxiv.org/html/2403.17888v3#bib.bib6); Hu et al., [2023](https://arxiv.org/html/2403.17888v3#bib.bib18)）解决了 NeRF 的混叠问题。此外，NeRF 的渲染效率通过蒸馏技术（Reiser et al., [2021](https://arxiv.org/html/2403.17888v3#bib.bib42); Yu et al., [2021](https://arxiv.org/html/2403.17888v3#bib.bib64)）和烘焙技术（Reiser et al., [2023](https://arxiv.org/html/2403.17888v3#bib.bib43); Hedman et al., [2021](https://arxiv.org/html/2403.17888v3#bib.bib17); Yariv et al., [2023](https://arxiv.org/html/2403.17888v3#bib.bib61); Chen et al., [2023a](https://arxiv.org/html/2403.17888v3#bib.bib11)）也得到了显著改善。此外，借助基于特征网格的场景表示（Chen et al., [2022](https://arxiv.org/html/2403.17888v3#bib.bib9); Müller et al., [2022](https://arxiv.org/html/2403.17888v3#bib.bib36); Liu et al., [2020](https://arxiv.org/html/2403.17888v3#bib.bib31); Sun et al., [2022a](https://arxiv.org/html/2403.17888v3#bib.bib50); Chen et al., [2023c](https://arxiv.org/html/2403.17888v3#bib.bib12); Fridovich-Keil et al., [2022](https://arxiv.org/html/2403.17888v3#bib.bib13)），NeRF 的训练和表示能力得到了提升。### 翻译
最近，3D 高斯点洒技术 (3DGS) (Kerbl et al., [2023](https://arxiv.org/html/2403.17888v3#bib.bib23)) 迅速崛起，展示了令人印象深刻的实时 NVS 结果。这种方法已迅速扩展到多个领域 (Yu et al., [2024](https://arxiv.org/html/2403.17888v3#bib.bib66); Yan et al., [2023](https://arxiv.org/html/2403.17888v3#bib.bib58); Zielonka et al., [2023](https://arxiv.org/html/2403.17888v3#bib.bib71); Xie et al., [2023](https://arxiv.org/html/2403.17888v3#bib.bib57))。在本研究中，我们提出将 3D 高斯“扁平化”为 2D 高斯原语，以更好地与物体表面对齐。结合两种新颖的正则化损失，我们的方法在重建表面方面比 3DGS 更加准确，同时保留其高质量和实时渲染能力。### 翻译
###  2.2. 3D 重建### 翻译
从多视角图像进行 3D 重建一直是计算机视觉中的一个长期目标。基于多视角立体的方法 (Schönberger et al., [2016](https://arxiv.org/html/2403.17888v3#bib.bib46); Yao et al., [2018](https://arxiv.org/html/2403.17888v3#bib.bib59); Yu and Gao, [2020](https://arxiv.org/html/2403.17888v3#bib.bib67)) 依赖于一个模块化的流程，包括特征匹配、深度预测和融合。相较之下，最近的神经网络方法 (Niemeyer et al., [2020](https://arxiv.org/html/2403.17888v3#bib.bib37); Yariv et al., [2020](https://arxiv.org/html/2403.17888v3#bib.bib62)) 通过 MLP 隐式表示表面 (Park et al., [2019](https://arxiv.org/html/2403.17888v3#bib.bib39); Mescheder et al., [2019](https://arxiv.org/html/2403.17888v3#bib.bib33))，并通过 Marching Cube 算法在训练后提取表面。进一步的进展 (Oechsle et al., [2021](https://arxiv.org/html/2403.17888v3#bib.bib38); Wang et al., [2021](https://arxiv.org/html/2403.17888v3#bib.bib52); Yariv et al., [2021](https://arxiv.org/html/2403.17888v3#bib.bib60)) 将隐式表面与体积渲染相结合，从 RGB 图像中实现了详细的表面重建。这些方法已经通过额外的正则化 (Yu et al., [2022b](https://arxiv.org/html/2403.17888v3#bib.bib68); Li et al., [2023](https://arxiv.org/html/2403.17888v3#bib.bib29); Yu et al., [2022a](https://arxiv.org/html/2403.17888v3#bib.bib65)) 扩展到大规模重建，并且在物体重建方面 (Wang et al., [2023](https://arxiv.org/html/2403.17888v3#bib.bib53)) 提高了效率。尽管这些令人印象深刻的发展，高效的大规模场景重建仍然是一项挑战。例如，Neuralangelo (Li et al., [2023](https://arxiv.org/html/2403.17888v3#bib.bib29)) 重建 Tanks and Temples 数据集中的单个场景需要 128 个 GPU 小时。在本研究中，我们引入了 2D 高斯喷溅，这是一种显著加速重建过程的方法。与之前的隐式神经表面表示相比，它实现了类似或稍微更好的结果，同时速度快了一个数量级。### 2.3. 可微分的点基图形### 翻译
可微分的点基渲染 (Insafutdinov 和 Dosovitskiy, [2018](https://arxiv.org/html/2403.17888v3#bib.bib19); Yifan 等, [2019](https://arxiv.org/html/2403.17888v3#bib.bib63); Aliev 等, [2020](https://arxiv.org/html/2403.17888v3#bib.bib2); Wiles 等, [2020](https://arxiv.org/html/2403.17888v3#bib.bib56); Rückert 等, [2022](https://arxiv.org/html/2403.17888v3#bib.bib44)) 因其在表示复杂结构方面的高效性和灵活性而被广泛探索。值得注意的是，NPBG (Aliev 等, [2020](https://arxiv.org/html/2403.17888v3#bib.bib2)) 将点云特征光栅化到图像平面上，随后利用卷积神经网络进行 RGB 图像预测。DSS (Yifan 等, [2019](https://arxiv.org/html/2403.17888v3#bib.bib63)) 专注于在已知照明条件下优化来自多视角图像的定向点云。Pulsar (Lassner 和 Zollhofer, [2021](https://arxiv.org/html/2403.17888v3#bib.bib28)) 引入了一种基于平铺的加速结构，以实现更高效的光栅化。最近，3DGS (Kerbl 等, [2023](https://arxiv.org/html/2403.17888v3#bib.bib23)) 优化了各向异性 3D 高斯原语，展示了实时光照逼真 NVS 结果。尽管取得了这些进展，但从无约束的多视图图像中使用基于点的表示仍然具有挑战性。本文展示了使用 2D 高斯原语的详细表面重建。同时，我们强调了额外正则化损失在优化过程中的关键作用，展示了它们对重建质量的重大影响。### 翻译
###  2.4. 同期工作### 翻译
自从 3DGS (Kerbl et al., [2023](https://arxiv.org/html/2403.17888v3#bib.bib23)) 被提出以来，它已在多个领域迅速适应。我们现在回顾与逆向渲染最相关的研究。这些研究 (Liang et al., [2023](https://arxiv.org/html/2403.17888v3#bib.bib30); Gao et al., [2023](https://arxiv.org/html/2403.17888v3#bib.bib15); Jiang et al., [2023](https://arxiv.org/html/2403.17888v3#bib.bib21); Shi et al., [2023](https://arxiv.org/html/2403.17888v3#bib.bib48)) 通过将法线建模为 3D 高斯原语的附加属性扩展了 3DGS。相较而言，我们的方法本质上通过使用 2D 高斯原语表示 3D 表面的切空间来定义法线，使其与基础几何结构更紧密对齐。此外，上述研究主要集中于估计场景的材料属性，并评估它们在重光照任务中的结果。值得注意的是，这些研究中没有一项专门针对表面重建，这是我们工作的主要焦点。### 翻译
我们还强调了我们的方法与同时期工作的区别，特别是 SuGaR (Guédon 和 Lepetit, [2023](https://arxiv.org/html/2403.17888v3#bib.bib16)) 和 NeuSG (Chen 等, [2023b](https://arxiv.org/html/2403.17888v3#bib.bib10))。与使用 3D 高斯近似 2D 高斯的 SuGaR 不同，我们的方法直接采用 2D 高斯，从而简化了流程，并在不进行额外网格细化的情况下提升了生成几何体的效果。NeuSG 联合优化 3D 高斯原件和隐式 SDF 网络，并从 SDF 网络提取表面，而我们的方法则利用 2D 高斯原件进行表面近似，提供了一种更快且在概念上更简单的解决方案。### 翻译
##  3\. 3D 高斯散点法### 翻译
Kerbl 等人 (Kerbl et al., [2023](https://arxiv.org/html/2403.17888v3#bib.bib23)) 提出通过 3D 高斯原语表示 3D 场景，并使用可微分体积溅射渲染图像。具体而言，3DGS 明确地通过 3D 协方差矩阵 𝚺𝚺\boldsymbol{\Sigma}bold_Σ 和它们的位置 𝐩ksubscript𝐩𝑘\mathbf{p}_{k}bold_p start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT 参数化高斯原语：### 翻译
(1) |  | 𝒢(𝐩)=exp(−12(𝐩−𝐩k)⊤𝚺−1(𝐩−𝐩k))  
其中协方差矩阵  
𝚺=𝐑𝐒𝐒⊤𝐑⊤  
\boldsymbol{\Sigma}=\mathbf{R}\mathbf{S}\mathbf{S}^{\top}\mathbf{R}^{\top}  
分解为一个缩放矩阵 𝐒\mathbf{S} 和一个旋转矩阵 𝐑\mathbf{R}。为了渲染图像，3D 高斯被转换为相机坐标系，通过世界到相机的变换矩阵 𝐖\mathbf{W} 并通过局部仿射变换 𝐉\mathbf{J} 投影到图像平面 (Zwicker et al., [2001a](https://arxiv.org/html/2403.17888v3#bib.bib72)):### 翻译
(2) |  | 𝚺′=𝐉𝐖⁢𝚺⁢𝐖⊤⁢𝐉⊤  
通过跳过 𝚺′ 的第三行和第三列，我们可以得到一个二维高斯分布 𝒢2⁢D，具有协方差矩阵 𝚺2⁢D。接下来，3DGS (Kerbl et al., [2023](https://arxiv.org/html/2403.17888v3#bib.bib23)) 采用体积 alpha 混合方法，从前到后整合 alpha 加权的外观：### 翻译
(3) |  | 𝐜⁢(𝐱)=∑k=1K𝐜k⁢αk⁢𝒢k2⁢D⁢(𝐱)⁢∏j=1k−1(1−αj⁢𝒢j2⁢D⁢(𝐱))  
其中 k 是高斯基元的索引，αk 表示阿尔法值，𝐜k 是视图依赖的外观。三维高斯基元的属性通过光度损失进行优化。### 翻译
#### 表面重建中的挑战### 翻译
重建表面使用 3D 高斯建模和点云面临几个挑战。首先，3D 高斯的体积辐射表示与表面的薄特点存在矛盾。其次，3DGS 本身并不建模表面法线，而法线在高质量表面重建中是至关重要的。第三，3DGS 中的光栅化过程缺乏多视图一致性，导致不同视角下的 2D 交点平面变化（Keselman 和 Hebert， [2023](https://arxiv.org/html/2403.17888v3#bib.bib25)），如图 [2](https://arxiv.org/html/2403.17888v3#S1.F2 "Figure 2 ‣ 1. Introduction ‣ 2D Gaussian Splatting for Geometrically Accurate Radiance Fields") (a) 所示。此外，使用仿射矩阵将 3D 高斯转换为光线空间仅能在中心附近获得准确的投影，周围区域的透视准确性受到牺牲（Zwicker 等， [2004](https://arxiv.org/html/2403.17888v3#bib.bib74)）。因此，这通常会导致噪声重建，如图 [5](https://arxiv.org/html/2403.17888v3#S6.F5 "Figure 5 ‣ Mesh Extraction ‣ 6.1. Implementation ‣ 6. Experiments ‣ 2D Gaussian Splatting for Geometrically Accurate Radiance Fields") 所示。### 翻译
##  4\. 2D 高斯点散射### 翻译
为了在高质量新视图合成的同时准确重建几何体，我们提出了可微分的 2D 高斯点云溅射（2DGS）。### 4.1. 建模### 翻译
与 3DGS (Kerbl 等人, [2023](https://arxiv.org/html/2403.17888v3#bib.bib23)) 采用在一个大块中建模整个角辐射不同，我们通过采用嵌入在 3D 空间中的“平面”2D 高斯来简化三维建模。通过 2D 高斯建模，原始对象在一个平面圆盘内分布密度，定义法线为密度变化最陡的方向。这一特性使得我们能够更好地与薄表面对齐。尽管先前的方法 (Kopanas 等人, [2021](https://arxiv.org/html/2403.17888v3#bib.bib27); Yifan 等人, [2019](https://arxiv.org/html/2403.17888v3#bib.bib63)) 也利用 2D 高斯进行几何重建，但它们需要密集的点云或真实法线作为输入。相比之下，我们只需稀疏的校准点云和光度监督便可同时重建外观和几何形状。### 翻译
如图 [3](https://arxiv.org/html/2403.17888v3#S4.F3 "Figure 3 ‣ 4.1. Modeling ‣ 4. 2D Gaussian Splatting ‣ 2D Gaussian Splatting for Geometrically Accurate Radiance Fields") 所示，我们的 2D Splat 以其中心点 𝐩𝐤\mathbf{p}_{k} 为特征，具有两个主要切向量 𝐭𝐮\mathbf{t}_{u} 和 𝐭𝐯\mathbf{t}_{v}，以及一个缩放向量 𝐒=(𝑠𝑢,𝑠𝑣)𝐒\mathbf{S}=(s_{u},s_{v})，该向量控制 2D 高斯的方差。请注意，原始法线由两个正交切向量定义，表示为 𝐭𝑤=𝐭𝑢×𝐭𝑣\mathbf{t}_{w}=\mathbf{t}_{u}\times\mathbf{t}_{v}。我们可以将方向排列成一个 3×3 的旋转矩阵 𝐑=[𝐭𝑢,𝐭𝑣,𝐭𝑤]\mathbf{R}=[\mathbf{t}_{u},\mathbf{t}_{v},\mathbf{t}_{w}]，并将缩放因子排列成一个 3×3 的对角矩阵 𝐒\mathbf{S}，其最后一个元素为零。### 翻译
因此，二维高斯分布是在世界空间的局部切平面中定义的，其参数化为：### 翻译
(4) |  | P(u,v)=𝐩k+su⁢𝐭u⁢u+sv⁢𝐭v⁢v=𝐇(u,v,1,1)ᵀP(u,v)=\mathbf{p}_{k}+s_{u}\mathbf{t}_{u}u+s_{v}\mathbf{t}_{v}v=\mathbf{H}(u,v,1,1)^{\mathrm{T}}  
---|---|---  
(5) |  | 其中𝐇=[su⁢𝐭u sv⁢𝐭v 𝟎 𝐩k 0 0 0 1]=[𝐑𝐒 𝐩k 0 1] 其中 \mathbf{H}=\begin{bmatrix}s_{u}\mathbf{t}_{u}&s_{v}\mathbf{t}_{v}&\boldsymbol{0}&\mathbf{p}_{k}\\ 0&0&0&1\\ \end{bmatrix}=\begin{bmatrix}\mathbf{R}\mathbf{S}&\mathbf{p}_{k}\\ \boldsymbol{0}&1\\ \end{bmatrix}  
 
其中 𝐇∈ 4×4 代表 2D 高斯的几何的同质变换矩阵。对于点 𝐮=(u,v) 在 𝑢𝑣 空间中，其 2D 高斯值可以通过标准高斯来计算。### 翻译
(6) |  | 𝒢⁢(𝐮)=exp⁡(−u2+v22)𝒢𝐮superscript𝑢2superscript𝑣22\mathcal{G}(\mathbf{u})=\exp\left(-\frac{u^{2}+v^{2}}{2}\right)caligraphic_G ( bold_u ) = roman_exp ( - divide start_ARG italic_u start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT + italic_v start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG start_ARG 2 end_ARG ) |   
---|---|---|---  

中心 𝐩ksubscript𝐩𝑘\mathbf{p}_{k}bold_p start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT、缩放 (su,sv)subscript𝑠𝑢subscript𝑠𝑣(s_{u},s_{v})(
italic_s start_POSTSUBSCRIPT italic_u end_POSTSUBSCRIPT , italic_s start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT ) 和旋转 (𝐭u,𝐭v)subscript𝐭𝑢subscript𝐭𝑣(\mathbf{t}_{u},\mathbf{t}_{v})( bold_t start_POSTSUBSCRIPT italic_u end_POSTSUBSCRIPT , bold_t start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT ) 是可学习的参数。依据 3DGS (Kerbl et al., [2023](https://arxiv.org/html/2403.17888v3#bib.bib23))，每个 2D 高斯原语具有不透明度 α𝛼\alphaitalic_α 和视角依赖外观 c𝑐citalic_c，用球谐函数进行参数化。### 翻译
![参见标题](x2.png) 图 3: 2D 高斯溅射的示意图。  
2D 高斯溅射是由一个中心点 𝐩k\mathbf{p}_{k}、切向量 𝐭u\mathbf{t}_{u} 和 𝐭v\mathbf{t}_{v} 以及两个缩放因子 (sus_{u} 和 svs_{v}) 特征化的椭圆盘，控制着方差。它们的椭圆投影通过光线-溅射交点进行采样（参见章节 [4.2](https://arxiv.org/html/2403.17888v3#S4.SS2 "4.2. Splatting ‣ 4. 2D Gaussian Splatting ‣ 2D Gaussian Splatting for Geometrically Accurate Radiance Fields")）并通过在图像空间中进行阿尔法混合进行累积。2DGS 通过梯度下降重建表面属性，例如颜色、深度和法线。### 4.2. 撒点法### 翻译
一种常见的渲染 2D 高斯的方法是利用透视投影的仿射近似将 2D 高斯原语投影到图像空间（Zwicker et al., [2001b](https://arxiv.org/html/2403.17888v3#bib.bib73), [a](https://arxiv.org/html/2403.17888v3#bib.bib72)）。然而，如 (Zwicker et al., [2004](https://arxiv.org/html/2403.17888v3#bib.bib74)) 所指出，这种投影仅在高斯的中心处准确，随着距离中心的增加，近似误差也会增大。为了解决这个问题，Zwicker et al. 提出了基于齐次坐标的公式。具体而言，将 2D splat 投影到图像平面可以通过齐次坐标中的一般 2D 到 2D 映射来描述。设 𝐖∈4×4 𝐖44\mathbf{W}\in{4\times 4}bold_W ∈ 4 × 4 为从世界空间到屏幕空间的组合变换矩阵。因此，屏幕空间点可以通过以下方式获得。### 翻译
(7) |  | 𝐱=(𝑥𝑧,𝑦𝑧,𝑧,𝑧)𝑇=𝐖𝑃(𝑢,𝑣)=𝐖𝐻(𝑢,𝑣,1,1)𝑇𝐱，其中 𝐱\mathbf{x} 代表从相机发出的同质光线，该光线穿过像素 (𝑥,𝑦)(𝑥,𝑦) (𝑥,𝑦) 且在深度 𝑧𝑧𝑧 处与 splat 相交。为了对一个二维高斯进行光栅化，Zwicker 等人提出使用隐式方法将其锥体投影到屏幕空间中，公式为 𝐌=(𝐖𝐻)⁻¹\mathbf{M}=(\mathbf{W}\mathbf{H})^{-1}。然而，逆变换会引入数值不稳定性，尤其是在 splat 退化为线段时（即，从侧面查看时）。为了解决这个问题，以前的表面 splatting 渲染方法使用预定义的阈值丢弃这种病态变换 (Zwicker et al., [2004](https://arxiv.org/html/2403.17888v3#bib.bib74))。然而，这种方案在可微渲染框架中带来了挑战，因为阈值化可能导致不稳定的优化。为了解决这个问题，我们借鉴 (Sigg et al., [2006](https://arxiv.org/html/2403.17888v3#bib.bib49)) 的方法，利用显式的光线-splat 相交。### 翻译
#### 光线-点交点### 翻译
我们通过寻找三个非平行平面的交点，来高效定位光线与水平方的交点，这是一种最初为专用硬件设计的方法 (Weyrich et al., [2007](https://arxiv.org/html/2403.17888v3#bib.bib54))。给定图像坐标 𝐱=(x,y) 𝐱𝑥𝑦 \mathbf{x}=(x,y) bold_x = ( italic_x , italic_y )，我们将像素的光线参数化为两个正交平面的交点：x 平面和 y 平面。具体而言，x 平面由法向量 (−1,0,0) 和偏移量 x 𝑥 𝑥 italic_x 定义。x 平面可以表示为一个 4D 齐次平面 𝐡x=(−1,0,0,x)T 𝑥 \mathbf{h}_{x}=(-1,0,0,x)^{\mathrm{T}} bold_h start_POSTSUBSCRIPT italic_x end_POSTSUBSCRIPT = ( - 1 , 0 , 0 , italic_x ) start_POSTSUPERSCRIPT roman_T end_POSTSUPERSCRIPT。同样，y 平面为 𝐡y=(0,−1,0,y)T 𝑦 \mathbf{h}_{y}=(0,-1,0,y)^{\mathrm{T}} bold_h start_POSTSUBSCRIPT italic_y end_POSTSUBSCRIPT = ( 0 , - 1 , 0 , italic_y ) start_POSTSUPERSCRIPT roman_T end_POSTSUPERSCRIPT。因此，光线 𝐱=(x,y) 𝐱𝑥𝑦 \mathbf{x}=(x,y) bold_x = ( italic_x , italic_y ) 是由这两个平面的交点确定的。### 翻译
接下来，我们将两个平面转换为二维高斯原语的局部坐标系，即 u⁢v𝑢𝑣uvitalic_u italic_v 坐标系统。注意，使用变换矩阵 𝐌𝐌\mathbf{M}bold_M 对平面上的点进行变换，相当于使用逆转置 𝐌−Tsuperscript𝐌T\mathbf{M}^{-\mathrm{T}}bold_M 对齐次平面参数进行变换 (Blinn, [1977](https://arxiv.org/html/2403.17888v3#bib.bib7))。因此，应用 𝐌=(𝐖𝐇)−1𝐌superscript𝐖𝐇1\mathbf{M}=(\mathbf{W}\mathbf{H})^{-1}bold_M = (bold_WH ) start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT 相当于 (𝐖𝐇)Tsuperscript𝐖𝐇T(\mathbf{W}\mathbf{H})^{\mathrm{T}}( bold_WH ) start_POSTSUPERSCRIPT roman_T end_POSTSUPERSCRIPT，消除了显式的矩阵求逆并得出：### 翻译
(8) |  | 𝐡u=(𝐖𝐇)T⁢𝐡x 𝐡v=(𝐖𝐇)T⁢𝐡y
公式序列 
𝐡𝑢上标𝐖𝐇T下标𝐡𝑥下标𝐡𝑣上标𝐖𝐇T下标𝐡𝑦
\displaystyle \mathbf{h}_{u}=(\mathbf{W}\mathbf{H})^{\mathrm{T}}{\mathbf{h}_{x}} \quad \mathbf{h}_{v}=(\mathbf{W}\mathbf{H})^{\mathrm{T}}{\mathbf{h}_{y}} 
粗体_h 开头_POST下标 斜体_u 结束_POST下标 = ( 粗体_WH ) 开头_POST上标 斜体_T 结束_POST上标 粗体_h 开头_POST下标 斜体_x 结束_POST下标 
粗体_h 开头_POST下标 斜体_v 结束_POST下标 = ( 粗体_WH ) 开头_POST上标 斜体_T 结束_POST上标 粗体_h 开头_POST下标 斜体_y 结束_POST下标 |   
---|---|---|---  
  
如在第 [4.1](https://arxiv.org/html/2403.17888v3#S4.SS1 "4.1. 建模 ‣ 4. 2D 高斯溅射 ‣ 2D 高斯溅射用于几何准确的辐射场") 节中介绍，2D 高斯平面上的点表示为 (u,v,1,1)𝑢𝑣11(u,v,1,1)( 斜体_u , 斜体_v , 1 , 1 )。同时，交点应落在变换后的 x𝑥x 斜体_x 平面和 y𝑦y 斜体_y 平面上。因此，### 翻译
(9) |  | 𝐡u⋅(u,v,1,1)T=𝐡v⋅(u,v,1,1)T=0⋅下标𝐡𝑢上标𝑢𝑣11T⋅下标𝐡𝑣上标𝑢𝑣11T0\displaystyle\mathbf{h}_{u}\cdot(u,v,1,1)^{\mathrm{T}}=\mathbf{h}_{v}\cdot(u,v% ,1,1)^{\mathrm{T}}=0\bold_h \text{下标} \, \textit{u} \, \text{上标} \, \text{u} \, \text{v} \, 1 \, 1 \, \text{T} = \bold_h \text{下标} \, \textit{v} \, \text{上标} \, \text{u} \, \text{v} \, 1 \, 1 \, \text{T} = 0 |   
---|---|---|---  
  
这导致了交点的有效解
𝐮⁢(𝐱)𝐮𝐱\mathbf{u}(\mathbf{x})\bold_u ( \bold_x ):### 翻译
(10) |  | u(𝑥)=𝑕u2𝑕v4−𝑕u4𝑕v2𝑕u1𝑕v2−𝑕u2𝑕v1v(𝑥)=𝑕u4𝑕v1−𝑕u1𝑕v4𝑕u1𝑕v2−𝑕u2𝑕v1公式序列𝑢𝑥上标下标𝑕𝑢2上标下标𝑕𝑣4上标下标𝑕𝑢4上标下标𝑕𝑣2上标下标𝑕𝑢1上标下标𝑕𝑣2上标下标𝑕𝑢2上标下标𝑕𝑣1𝑣𝑥上标下标𝑕𝑢4上标下标𝑕𝑣1上标下标𝑕𝑢1上标下标𝑕𝑣4上标下标𝑕𝑢1上标下标𝑕𝑣2上标下标𝑕𝑢2上标下标𝑕𝑣1
\displaystyle u(\mathbf{x})=\frac{\mathbf{h}_{u}^{2}\mathbf{h}_{v}^{4}-\mathbf{h}_{u}^{4}\mathbf{h}_{v}^{2}}{\mathbf{h}_{u}^{1}\mathbf{h}_{v}^{2}-\mathbf{h}_{u}^{2}\mathbf{h}_{v}^{1}}\qquad v(\mathbf{x})=\frac{\mathbf{h}_{u}^{4}\mathbf{h}_{v}^{1}-\mathbf{h}_{u}^{1}\mathbf{h}_{v}^{4}}{\mathbf{h}_{u}^{1}\mathbf{h}_{v}^{2}-\mathbf{h}_{u}^{2}\mathbf{h}_{v}^{1}} 其中 𝑕u𝑖,𝑕v𝑖\mathbf{h}_{u}^{i},\mathbf{h}_{v}^{i} 是四维平面的第 𝑖 个参数。请注意 𝑕u3\mathbf{h}_{u}^{3} 和 𝑕v3\mathbf{h}_{v}^{3} 根据公式 [5] 始终为零。 一旦获得局部坐标 (𝑢,𝑣)(𝑢,𝑣)，我们可以使用公式 [7] 计算交点的深度 𝑧，并利用公式 [6] 评估高斯值。### 翻译
#### 退化解法### 翻译
当从倾斜的视角观察一个二维高斯分布时，它会退化为屏幕空间中的一条线。因此，在光栅化过程中可能会被忽略。为了处理这些情况并稳定优化，我们采用了 (Botsch et al., [2005](https://arxiv.org/html/2403.17888v3#bib.bib8)) 中提出的对象空间低通滤波器：### 翻译
(11) |  | 𝒢^⁢(𝐱)=max⁡{𝒢⁢(𝐮⁢(𝐱)),𝒢⁢(𝐱−𝐜σ)}^𝒢𝐱𝒢𝐮𝐱𝒢𝐱𝐜𝜎\hat{\mathcal{G}}(\mathbf{x})=\max\left\{\mathcal{G}(\mathbf{u}(\mathbf{x})), \mathcal{G}\left(\frac{\mathbf{x}-\mathbf{c}}{\sigma}\right)\right\} 其中 𝐮⁢(𝐱)𝐮𝐱\mathbf{u}(\mathbf{x}) 是由公式 [10](https://arxiv.org/html/2403.17888v3#S4.E10 "In Ray-splat Intersection ‣ 4.2. Splatting ‣ 4. 2D Gaussian Splatting ‣ 2D Gaussian Splatting for Geometrically Accurate Radiance Fields") 给出的，且 𝐜𝐜\mathbf{c} 是中心 𝐩ksubscript𝐩𝑘\mathbf{p}_{k} 的投影。在直观上，𝒢^⁢(𝐱)^𝒢𝐱\hat{\mathcal{G}}(\mathbf{x}) 是由中心为 𝐜𝐜\mathbf{c} 且半径为 σ𝜎\sigma 的固定屏幕空间高斯低通滤波器下界的。在我们的实验中，我们将 σ=2/2𝜎22\sigma=\sqrt{2}/2 来确保渲染过程中使用了足够的像素。### 翻译
#### 光栅化### 翻译
我们遵循与 3DGS (Kerbl et al., [2023](https://arxiv.org/html/2403.17888v3#bib.bib23)) 中类似的光栅化过程。首先，计算每个高斯原语的屏幕空间边界框。然后，根据它们中心的深度对 2D 高斯进行排序，并根据它们的边界框组织成瓦片。最后，使用体积 alpha 混合从前到后整合带 alpha 权重的外观：### 翻译
(12) |  | 𝐜⁢(𝐱)=∑i=1𝐜i⁢αi⁢𝒢^i⁢(𝐮⁢(𝐱))⁢∏j=1i−1(1−αj⁢𝒢^j⁢(𝐮⁢(𝐱)))  |   
---|---|---|---  
\[\mathbf{c}(\mathbf{x})=\sum_{i=1}\mathbf{c}_{i}\,\alpha_{i}\,\hat{\mathcal{G}}_{i}(\mathbf{u}(\mathbf{x}))\prod_{j=1}^{i-1}(1-\alpha_{j}\,\hat{\mathcal{G}}_{j}(\mathbf{u}(\mathbf{x})))\]  
过程在累积的不透明度达到饱和时终止。### 翻译
图 4: 我们的方法、3DGS (Kerbl et al., [2023](https://arxiv.org/html/2403.17888v3#bib.bib23)) 和 SuGaR (Guédon 和 Lepetit, [2023](https://arxiv.org/html/2403.17888v3#bib.bib16)) 之间的视觉比较 (测试集视图)，使用来自真实世界数据集的场景 (Barron et al., [2022b](https://arxiv.org/html/2403.17888v3#bib.bib5))。我们的方法在合成几何准确的辐射场和表面重建方面表现出色，在捕捉锐利边缘和复杂细节时超过了 3DGS 和 SuGaR。### 翻译
##  5. 训练### 翻译
我们提出的 2D 高斯方法在几何建模方面有效，但仅通过光度损失进行优化时可能导致重建结果噪声较大，这是 3D 重建任务固有的挑战（Barron et al., [2022b](https://arxiv.org/html/2403.17888v3#bib.bib5); Zhang et al., [2020](https://arxiv.org/html/2403.17888v3#bib.bib69); Yu et al., [2022b](https://arxiv.org/html/2403.17888v3#bib.bib68)）。为了缓解这一问题并改善几何重建，我们引入了两个正则化项：深度扭曲和法向一致性。### 翻译
#### 深度失真### 翻译
与 NeRF 不同，3DGS 的体积渲染并不考虑相交的高斯原语之间的距离。因此，分散的高斯可能会导致相似的颜色和深度渲染。这与表面渲染有所不同，后者的光线仅与第一个可见表面相交一次。为了解决这一问题，我们从 Mip-NeRF360 (Barron et al., [2022a](https://arxiv.org/html/2403.17888v3#bib.bib4)) 中获得启发，提出了一种深度扭曲损失，通过最小化光线-斑点交点之间的距离，以集中光线沿途的权重分布：### 翻译
(13) |  | ℒd=∑i,jωi⁢ωj⁢|zi−zj|  
---|---|---|---  
其中  
ωi=αi⁢𝒢^i⁢(𝐮⁢(𝐱))⁢∏j=1i−1(1−αj⁢𝒢^j⁢(𝐮⁢(𝐱)))  
ω_{i}=\alpha_{i}\,\hat{\mathcal{G}}_{i}(\mathbf{u}(\mathbf{x}))\prod_{j=1}^{i-1}(1-\alpha_{j}\,\hat{\mathcal{G}}_{j}(\mathbf{u}(\mathbf{x})))是第 i 个交点的混合权重，z_{i}是交点的深度。与 Mip-NeRF360 中的失真损失不同，z_{i} 是采样点之间的距离并未优化，我们的方法通过调整交点深度 z_{i} 直接鼓励点的聚集。注意，我们以类似 (Sun et al., [2022b](https://arxiv.org/html/2403.17888v3#bib.bib51)) 的方式高效地使用 CUDA 实现此正则化项。### 翻译
表1: 在 DTU 数据集上的定量比较 (Jensen et al., [2014](https://arxiv.org/html/2403.17888v3#bib.bib20))。我们的 2DGS 方法在其他方法中实现了最高的重建精度，并且与基于 SDF 的基线相比，提供了 100×100\times100 的速度提升。### 翻译
| 24 | 37 | 40 | 55 | 63 | 65 | 69 | 83 | 97 | 105 | 106 | 110 | 114 | 118 | 122 |  | 平均值 | 时间  
---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---  
隐式 | NeRF (Mildenhall 等, [2021](https://arxiv.org/html/2403.17888v3#bib.bib35)) |  | 1.90 | 1.60 | 1.85 | 0.58 | 2.28 | 1.27 | 1.47 | 1.67 | 2.05 | 1.07 | 0.88 | 2.53 | 1.06 | 1.15 | 0.96 |  | 1.49 | ≈ 12h  
VolSDF (Yariv 等, [2021](https://arxiv.org/html/2403.17888v3#bib.bib60)) |  | 1.14 | 1.26 | 0.81 | 0.49 | 1.25 | 0.70 | 0.72 | 1.29 | 1.18 | 0.70 | 0.66 | 1.08 | 0.42 | 0.61 | 0.55 |  | 0.86 | ≈ 12h  
NeuS (Wang 等, [2021](https://arxiv.org/html/2403.17888v3#bib.bib52)) |  | 1.00 | 1.37 | 0.93 | 0.43 | 1.10 | 0.65 | 0.57 | 1.48 | 1.09 | 0.83 | 0.52 | 1.20 | 0.35 | 0.49 | 0.54 |  | 0.84 | ≈ 12h  
显式 | 3DGS (Kerbl 等, [2023](https://arxiv.org/html/2403.17888v3#bib.bib23)) |  | 2.14 | 1.53 | 2.08 | 1.68 | 3.49 | 2.21 | 1.43 | 2.07 | 2.22 | 1.75 | 1.79 | 2.55 | 1.53 | 1.52 | 1.50 |  | 1.96 | 11.2 m  
SuGaR (Guédon 和 Lepetit, [2023](https://arxiv.org/html/2403.17888v3#bib.bib16)) |  | 1.47 | 1.33 | 1.13 | 0.61 | 2.25 | 1.71 | 1.15 | 1.63 | 1.62 | 1.07 | 0.79 | 2.45 | 0.98 | 0.88 | 0.79 |  | 1.33 |  ∼类似于 ∼ 1h  
2DGS-15k (我们) |  | 0.48 | 0.92 | 0.42 | 0.40 | 1.04 | 0.83 | 0.83 | 1.36 | 1.27 | 0.76 | 0.72 | 1.63 | 0.40 | 0.76 | 0.60 |  | 0.83 | 5.5 m  
| 2DGS-30k (我们) |  | 0.48 | 0.91 | 0.39 | 0.39 | 1.01 | 0.83 | 0.81 | 1.36 | 1.27 | 0.76 | 0.70 | 1.40 | 0.40 | 0.76 | 0.52 |  | 0.80 | 10.9 m  
  
#### 正常一致性### 翻译
由于我们的表示基于二维高斯表面元素，我们必须确保所有二维光斑在局部上与实际表面对齐。在体积渲染的背景下，沿射线可能存在多个半透明表面，我们考虑在交点的中位点 𝐩𝑠（\mathbf{p}_{s}）的位置，此时累计的不透明度达到0.5。然后，我们将光斑的法线与深度图的梯度对齐，如下所示：### 翻译
(14) |  | ℒn=∑iωi⁢(1−𝐧iT⁢𝐍)  
其中 i 𝑖 i 索引与光线相交的 splat，  
ω 𝜔 ω 表示交点的融合权重，  
𝐧subscript𝐧𝑖 \mathbf{n}_{i} 表示朝向摄像机的 splat 法向量，并且 𝐍 \mathbf{N} 是由深度图的梯度估计得到的法向量。具体而言， 𝐍 \mathbf{N} 是通过有限差分法从附近的深度点计算得到的，具体如下：### 翻译
(15) |  | 𝐍⁢(x,y)=∇x𝐩s×∇y𝐩s|∇x𝐩s×∇y𝐩s|𝐍𝑥𝑦\mathbf{N}(x,y)=\frac{\nabla_{x}\mathbf{p}_{s}\times\nabla_{y}\mathbf{p}_{s}}{% |\nabla_{x}\mathbf{p}_{s}\times\nabla_{y}\mathbf{p}_{s}|}  
---|---|---|---  
  
通过将 splat 法线与估计的表面法线对齐，我们确保 2D splats 在局部上近似实际的物体表面。### 翻译
#### 最终损失### 翻译
最后，我们使用一组已定位的图像从初始稀疏点云中优化我们的模型。我们最小化以下损失函数：### 翻译
(16) |  | ℒ=ℒc+α⁢ℒd+β⁢ℒn  
ℒ=ℒc+αℒd+βℒn  
其中 ℒc 是结合了 RGB 重建损失 ℒ1 和来自 (Kerbl et al., [2023](https://arxiv.org/html/2403.17888v3#bib.bib23) ) 的 D-SSIM 项，而 ℒd 和 ℒn 是正则化项。我们设定 α=1000 对于有界场景，α=100 对于无界场景，以及 β=0.05 适用于所有场景。### 翻译
## 6. 实验### 翻译
我们现在展示我们 2D 高斯溅射重建方法的评估，包括与之前的最先进隐式和显式方法在外观和几何上的比较。接着，我们分析所提议组件的贡献。###  6.1. 实现### 翻译
我们使用自定义的 CUDA 内核实现我们的 2D 高斯溅射，基于 3DGS 的框架（Kerbl et al.， [2023](https://arxiv.org/html/2403.17888v3#bib.bib23)）。我们扩展了渲染器，以输出深度失真图、深度图和法线图用于正则化（详细计算见补充材料的附录 A 和 B）。在训练过程中，我们根据 3DGS 中的自适应控制策略增加 2D 高斯原语的数量。由于我们的方法并不直接依赖于投影的 2D 中心的梯度，因此我们将 3D 中心 𝐩𝐤 \mathbf{p}_{k} 投影到屏幕空间，以作为一种近似。类似地，我们使用 0.0002 的梯度阈值，并在每 3000 步中移除不透明度低于 0.05 的溅射。我们在单个 GTX RTX3090 GPU 上进行了所有实验。### 翻译
#### 网格提取### 翻译
为了从重建的 2D splats 中提取网格，我们使用投影到像素的 splats 深度值渲染训练视角的深度图，并利用截断有符号距离融合 (TSDF) 来融合重建深度图，使用 Open3D (Zhou et al., [2018](https://arxiv.org/html/2403.17888v3#bib.bib70))。我们将体素大小设置为 0.0040.0040.0040.004，并在 TSDF 融合过程中将截断阈值设置为 0.02。为了公平比较，我们还扩展了原始的 3DGS 以渲染深度，并采用相同的技术进行表面重建。### 翻译
表2: Tanks and Temples 数据集上的定量结果 (Knapitsch et al., [2017](https://arxiv.org/html/2403.17888v3#bib.bib26))。我们报告了 F1 分数和训练时间。### 翻译
| NeuS | Geo-Neus | Neurlangelo | SuGaR | 3DGS | 我们的方法  
---|---|---|---|---|---  
Barn | 0.29 | 0.33 | 0.70 | 0.14 | 0.13 | 0.41  
Caterpillar | 0.29 | 0.26 | 0.36 | 0.16 | 0.08 | 0.23  
Courthouse | 0.17 | 0.12 | 0.28 | 0.08 | 0.09 | 0.16  
Ignatius | 0.83 | 0.72 | 0.89 | 0.33 | 0.04 | 0.51  
Meetingroom | 0.24 | 0.20 | 0.32 | 0.15 | 0.01 | 0.17  
Truck | 0.45 | 0.45 | 0.48 | 0.26 | 0.19 | 0.45  
Mean | 0.38 | 0.35 | 0.50 | 0.19 | 0.09 | 0.32  
Time | 约 24h | 约 24h | 约 24h | 约 1h | 14.3 分钟 | 15.5 分钟  
  
表 3: 在 DTU 数据集上（Jensen et al., [2014](https://arxiv.org/html/2403.17888v3#bib.bib20)）的 2DGS（我们的方法）、3DGS 和 SuGaR 的性能比较。我们报告了平均的 Chamfer 距离、PSNR（训练集视图）、重建时间和模型大小。### 翻译
| CD ↓↓\downarrow↓ | PSNR ↑↑\uparrow↑ | 时间 ↓↓\downarrow↓ | MB (存储) ↓↓\downarrow↓  
---|---|---|---|---  
3DGS (Kerbl et al., [2023](https://arxiv.org/html/2403.17888v3#bib.bib23)) | 1.96 | 35.76 | 11.2 m | 113  
SuGaR (Guédon and Lepetit, [2023](https://arxiv.org/html/2403.17888v3#bib.bib16)) | 1.33 | 34.57 |  ∼similar-to\sim∼1 h | 1247  
2DGS-15k (我们的方法) | 0.83 | 33.42 | 5.5 m | 52  
2DGS-30k (我们的方法) | 0.80 | 34.52 | 10.9 m | 52  
  
![Refer to caption](extracted/6224839/figures/dtu.png) 图 5: 在 DTU 基准上的定性比较 (Jensen et al., [2014](https://arxiv.org/html/2403.17888v3#bib.bib20))。我们的 2DGS 生成了详细且无噪声的表面。表 4: Mip-NeRF 360 (Barron et al., [2022a](https://arxiv.org/html/2403.17888v3#bib.bib4)) 数据集上的定量结果。基线方法的所有分数均直接取自其论文（如果可用）。我们报告了 3DGS、SuGaR 及我们的性能，使用 30⁢k30𝑘30k30 italic_k 迭代。### 翻译
| 户外场景 | 室内场景  
---|---|---  
| PSNR ↑↑\uparrow↑ | SSIM ↑↑\uparrow↑ | LIPPS ↓↓\downarrow↓ | PSNR ↑↑\uparrow↑ | SSIM ↑↑\uparrow↑ | LIPPS ↓↓\downarrow↓  
NeRF | 21.46 | 0.458 | 0.515 | 26.84 | 0.790 | 0.370  
Deep Blending | 21.54 | 0.524 | 0.364 | 26.40 | 0.844 | 0.261  
Instant NGP | 22.90 | 0.566 | 0.371 | 29.15 | 0.880 | 0.216  
MERF | 23.19 | 0.616 | 0.343 | 27.80 | 0.855 | 0.271  
BakedSDF | 22.47 | 0.585 | 0.349 | 27.06 | 0.836 | 0.258  
MipNeRF360 | 24.47 | 0.691 | 0.283 | 31.72 | 0.917 | 0.180  
Mobile-NeRF | 21.95 | 0.470 | 0.470 | - | - | -  
SuGaR | 22.93 | 0.629 | 0.356 | 29.43 | 0.906 | 0.225  
3DGS | 24.64 | 0.731 | 0.234 | 30.41 | 0.920 | 0.189  
2DGS (我们的) | 24.34 | 0.717 | 0.246 | 30.40 | 0.916 | 0.195  
  
###  6.2. 比较### 翻译
#### 数据集### 翻译
我们在多个数据集上评估了我们方法的性能，包括 DTU (Jensen 等，[2014](https://arxiv.org/html/2403.17888v3#bib.bib20))，Tanks and Temples (Knapitsch 等，[2017](https://arxiv.org/html/2403.17888v3#bib.bib26))，以及 Mip-NeRF360 (Barron 等，[2022a](https://arxiv.org/html/2403.17888v3#bib.bib4))。DTU 数据集包含 15 个场景，每个场景都有 49 或 69 张分辨率为 1600×1200 的图像。我们使用 Colmap (Schönberger 和 Frahm, [2016](https://arxiv.org/html/2403.17888v3#bib.bib45)) 为每个场景生成稀疏点云，并将图像降采样到 800×600 的分辨率以提高效率。我们对 3DGS (Kerbl 等，[2023](https://arxiv.org/html/2403.17888v3#bib.bib23)) 和 SuGaR (Guédon 和 Lepetit, [2023](https://arxiv.org/html/2403.17888v3#bib.bib16)) 使用相同的训练过程以进行比较。### 翻译
#### 几何重建### 翻译
在表 1 和表 3 中，我们将我们的几何重建与当前最先进的隐式方法（即 NeRF (Mildenhall et al., [2020](https://arxiv.org/html/2403.17888v3#bib.bib34))、VolSDF (Yariv et al., [2021](https://arxiv.org/html/2403.17888v3#bib.bib60)) 和 NeuS (Wang et al., [2021](https://arxiv.org/html/2403.17888v3#bib.bib52))) 以及显式方法（即 3DGS (Kerbl et al., [2023](https://arxiv.org/html/2403.17888v3#bib.bib23)) 和同时进行的工作 SuGaR (Guédon and Lepetit, [2023](https://arxiv.org/html/2403.17888v3#bib.bib16))) 进行比较，使用 Chamfer 距离和 DTU 数据集进行训练时间评估。我们的算法在 Chamfer 距离方面优于所有比较的方法。此外，如表 2 所示，2DGS 在 TnT 数据集上与 SDF 模型（即 NeuS (Wang et al., [2021](https://arxiv.org/html/2403.17888v3#bib.bib52)) 和 Geo-Neus (Fu et al., [2022](https://arxiv.org/html/2403.17888v3#bib.bib14))) 的竞争结果相当，并且与显式重建方法（即 3DGS 和 SuGaR）相比具有显著更好的重建效果。值得注意的是，我们的模型在效率上表现出色，提供了大约比隐式重建方法快 100 倍的重建速度，且比同时进行的工作 SuGaR 快 3 倍以上。我们的方法还能够实现定性更好的重建，具有更多的外观和几何细节，并且少量的离群值，如图 5 所示。此外，基于 SDF 的重建方法需要预先定义球形尺寸进行初始化，而这对 SDF 重建的成功至关重要。相反，我们的方法利用基于辐射场的几何建模，对于初始化的敏感性较低。我们在图 9 和图 10 中包含了 DTU 和 TnT 的完整几何重建结果。### 翻译
#### 外观重建### 翻译
我们的方法将 3D 场景表示为辐射场，提供高质量的新视图合成。在这一部分，我们使用 Mip-NeRF360 数据集对我们的新视图渲染与基线方法进行比较，如表 [4](https://arxiv.org/html/2403.17888v3#S6.T4 "Table 4 ‣ Mesh Extraction ‣ 6.1. Implementation ‣ 6. Experiments ‣ 2D Gaussian Splatting for Geometrically Accurate Radiance Fields") 和图 [4](https://arxiv.org/html/2403.17888v3#S4.F4 "Figure 4 ‣ Rasterization ‣ 4.2. Splatting ‣ 4. 2D Gaussian Splatting ‣ 2D Gaussian Splatting for Geometrically Accurate Radiance Fields") 所示。需要注意的是，由于 Mip-NeRF360 数据集中没有真实的几何体数据，因此我们将重点放在定量比较上。值得注意的是，我们的方法在最先进的技术中始终实现了竞争力的新视图合成结果，同时提供了几何上准确的表面重建。我们在图 [11](https://arxiv.org/html/2403.17888v3#A4.F11 "Figure 11 ‣ Appendix D Additional Results ‣ 2D Gaussian Splatting for Geometrically Accurate Radiance Fields") 中包含了外观渲染结果。### 翻译
表 5: 关于 DTU 数据集的正则化项和网格提取方法的定量研究### 翻译
| 准确率 ↓↓\downarrow↓ | 完成度 ↓↓\downarrow↓ | 平均值 ↓↓\downarrow↓  
---|---|---|---  
A. 不考虑法线一致性 | 1.35 | 1.13 | 1.24  
B. 不考虑深度失真 | 0.89 | 0.87 | 0.88  
C. 考虑预期深度 | 0.88 | 1.01 | 0.94  
D. 考虑 SPSR | 1.25 | 0.89 | 1.07  
E. 完整模型 | 0.79 | 0.86 | 0.83  
  
![Refer to caption](x4.png) 图 6: 正则化效果的定性研究。从左到右 - 输入图像、没有法线一致性的表面法线、没有深度失真的表面法线以及我们的完整模型。禁用法线一致性损失会导致表面方向变得嘈杂；而省略深度失真正则化则会导致表面法线模糊。完整模型结合了这两种正则化，成功捕捉到了清晰和平坦的特征。### 6.3. 消融实验### 翻译
在本节中，我们将设计选择进行孤立分析，并测量它们对重建质量的影响，包括正则化项和网格提取。我们在DTU数据集（Jensen et al.，[2014](https://arxiv.org/html/2403.17888v3#bib.bib20)）上进行实验，进行 15k𝑘kitalic_k 次迭代，并报告重建准确性、完整性和平均重建质量。选择的定量效果在表 [5](https://arxiv.org/html/2403.17888v3#S6.T5 "Table 5 ‣ Appearance Reconstruction ‣ 6.2. Comparison ‣ 6. Experiments ‣ 2D Gaussian Splatting for Geometrically Accurate Radiance Fields") 中有所记录。更多的基线比较可以在补充材料的附录 C 中找到。### 翻译
#### 正则化### 翻译
我们首先检查所提出的法向一致性和深度失真正则化项的影响。我们的模型（表 [5](https://arxiv.org/html/2403.17888v3#S6.T5 "Table 5 ‣ Appearance Reconstruction ‣ 6.2. Comparison ‣ 6. Experiments ‣ 2D Gaussian Splatting for Geometrically Accurate Radiance Fields") E）在应用这两个正则化项时表现最佳。我们观察到，禁用法向一致性（表 [5](https://arxiv.org/html/2403.17888v3#S6.T5 "Table 5 ‣ Appearance Reconstruction ‣ 6.2. Comparison ‣ 6. Experiments ‣ 2D Gaussian Splatting for Geometrically Accurate Radiance Fields") A）可能会导致方向不正确，如图 [6](https://arxiv.org/html/2403.17888v3#S6.F6 "Figure 6 ‣ Appearance Reconstruction ‣ 6.2. Comparison ‣ 6. Experiments ‣ 2D Gaussian Splatting for Geometrically Accurate Radiance Fields") A 所示。此外，缺少深度失真（表 [5](https://arxiv.org/html/2403.17888v3#S6.T5 "Table 5 ‣ Appearance Reconstruction ‣ 6.2. Comparison ‣ 6. Experiments ‣ 2D Gaussian Splatting for Geometrically Accurate Radiance Fields") B）会导致表面噪声，如图 [6](https://arxiv.org/html/2403.17888v3#S6.F6 "Figure 6 ‣ Appearance Reconstruction ‣ 6.2. Comparison ‣ 6. Experiments ‣ 2D Gaussian Splatting for Geometrically Accurate Radiance Fields") B 所示。### 翻译
#### 网格提取### 翻译
我们现在分析网格提取的选择。我们的完整模型（表 5 E）利用 TSDF 融合进行网格提取，采用中位深度。一种替代选项是使用期望深度而非中位深度。然而，如表 5 C 所示，这会导致更差的重建，因为它对离群值更为敏感。此外，我们的方法在使用 2D 高斯的中心和法线作为输入的情况下，超越了筛选的泊松表面重建（SPSR）（表 5 D）（Kazhdan 和 Hoppe，[2013](https://arxiv.org/html/2403.17888v3#bib.bib22)），这是由于 SPSR 无法结合 2D 高斯基元的透明度和尺寸。### 翻译
##  7. 结论### 翻译
我们提出了一种二维高斯喷涂的方法，这是一种用于几何上准确的辐射场重建的新颖方法。我们利用二维高斯原语进行三维场景表示，从而促进了准确且视图一致的几何建模和渲染。我们提出了两种正则化技术，以进一步增强重建的几何效果。在多个挑战性数据集上的大量实验验证了我们方法的有效性和效率。### 翻译
#### 限制 사항### 翻译
虽然我们的方法成功地为各种对象和场景提供了准确的外观和几何重建，但我们也讨论了其局限性：首先，我们假设表面具有完全不透明性，并从多视角深度图中提取网格。这在准确处理半透明表面（例如玻璃）时可能会面临挑战，因为它们复杂的光传输特性，如图 [12] 所示。其次，我们当前的加密策略更偏向于丰富纹理的区域，而非丰富几何的区域，这偶尔会导致对精细几何结构的表示不够准确。更有效的加密策略可以缓解这个问题。最后，我们的正则化经常涉及图像质量与几何之间的权衡，并可能导致某些区域过度平滑。### 翻译
###### 致谢。### 翻译
BH 和 SG 得到了 NSFC #62172279, #61932020 的支持，以及上海学术研究领军人才计划的支持。ZY、AC 和 AG 得到了 ERC 启动基金 LEGO-3D (850533) 和 DFG EXC 编号 2064/1 - 项目编号 390727645 的支持。### 翻译
## 参考文献### 翻译
* (1)
  * Aliev 等 (2020) Kara-Ali Aliev, Artem Sevastopolsky, Maria Kolos, Dmitry Ulyanov, 和 Victor Lempitsky. 2020. 神经点基础图形. 在 _Computer Vision–ECCV 2020: 第 16 届欧洲会议, 格拉斯哥, 英国, 2020 年 8 月 23–28 日, 会议录, 第 XXII 部分 16_. Springer, 696–712.
  * Barron 等 (2021) Jonathan T. Barron, Ben Mildenhall, Matthew Tancik, Peter Hedman, Ricardo Martin-Brualla, 和 Pratul P. Srinivasan. 2021. Mip-NeRF: 一种用于抗锯齿神经辐射场的多尺度表示. _ICCV_ (2021).
  * Barron 等 (2022a) Jonathan T Barron, Ben Mildenhall, Dor Verbin, Pratul P Srinivasan, 和 Peter Hedman. 2022a. Mip-nerf 360: 无界抗锯齿神经辐射场. 在 _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_. 5470–5479.
  * Barron 等 (2022b) Jonathan T. Barron, Ben Mildenhall, Dor Verbin, Pratul P. Srinivasan, 和 Peter Hedman. 2022b. Mip-NeRF 360: 无界抗锯齿神经辐射场. _CVPR_ (2022).
  * Barron 等 (2023) Jonathan T. Barron, Ben Mildenhall, Dor Verbin, Pratul P. Srinivasan, 和 Peter Hedman. 2023. Zip-NeRF: 基于网格的抗锯齿神经辐射场. _ICCV_ (2023).
  * Blinn (1977) James F Blinn. 1977. 三维空间中线的均匀公式化. 在 _Proceedings of the 4th annual conference on Computer graphics and interactive techniques_. 237–241.
  * Botsch 等 (2005) Mario Botsch, Alexander Hornung, Matthias Zwicker, 和 Leif Kobbelt. 2005. 当今 GPU 上的高质量表面喷溅. 在 _Proceedings Eurographics/IEEE VGTC Symposium Point-Based Graphics, 2005._ IEEE, 17–141.
  * Chen 等 (2022) Anpei Chen, Zexiang Xu, Andreas Geiger, Jingyi Yu, 和 Hao Su. 2022. TensoRF: 张量辐射场. 在 _European Conference on Computer Vision (ECCV)_.
  * Chen 等 (2023b) Hanlin Chen, Chen Li, 和 Gim Hee Lee. 2023b. NeuSG: 具有 3D 高斯喷溅引导的神经隐式表面重建. _arXiv preprint arXiv:2312.00846_ (2023).
  * Chen 等 (2023a) Zhiqin Chen, Thomas Funkhouser, Peter Hedman, 和 Andrea Tagliasacchi. 2023a. Mobilenerf: 利用多边形光栅化管线在移动架构上高效渲染神经场. 在 _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_. 16569–16578.
  * Chen 等 (2023c) Zhang Chen, Zhong Li, Liangchen Song, Lele Chen, Jingyi Yu, Junsong Yuan, 和 Yi Xu. 2023c. NeuRBF: 一种具有自适应径向基函数的神经场表示. 在 _Proceedings of the IEEE/CVF International Conference on Computer Vision_. 4182–4194.
  * Fridovich-Keil 等 (2022) Sara Fridovich-Keil, Alex Yu, Matthew Tancik, Qinhong Chen, Benjamin Recht, 和 Angjoo Kanazawa. 2022. Plenoxels: 无需神经网络的辐射场. 在 _CVPR_.
  * Fu 等 (2022) Qiancheng Fu, Qingshan Xu, Yew-Soon Ong, 和 Wenbing Tao. 2022. Geo-Neus: 具有几何一致性的神经隐式表面学习用于多视图重建. _Advances in Neural Information Processing Systems (NeurIPS)_ (2022).
  * Gao 等 (2023) Jian Gao, Chun Gu, Youtian Lin, Hao Zhu, Xun Cao, Li Zhang, 和 Yao Yao. 2023. 可重光照的 3D 高斯: 具有 BRDF 分解和光线追踪的实时点云重光照. _arXiv:2311.16043_ (2023).
  * Guédon 和 Lepetit (2023) Antoine Guédon 和 Vincent Lepetit. 2023. SuGaR: 适用于高效 3D 网格重建和高质量网格渲染的表面对齐高斯喷溅. _arXiv preprint arXiv:2311.12775_ (2023).
  * Hedman 等 (2021) Peter Hedman, Pratul P Srinivasan, Ben Mildenhall, Jonathan T Barron, 和 Paul Debevec. 2021. 为实时视图合成制作神经辐射场. 在 _Proceedings of the IEEE/CVF International Conference on Computer Vision_. 5875–5884.
  * Hu 等 (2023) Wenbo Hu, Yuling Wang, Lin Ma, Bangbang Yang, Lin Gao, Xiao Liu, 和 Yuewen Ma. 2023. Tri-MipRF: 高效抗锯齿神经辐射场的三倍缩放表示. 在 _ICCV_.
  * Insafutdinov 和 Dosovitskiy (2018) Eldar Insafutdinov 和 Alexey Dosovitskiy. 2018. 具有可微点云的无监督形状和姿态学习. _Advances in neural information processing systems_ 31 (2018).
  * Jensen 等 (2014) Rasmus Jensen, Anders Dahl, George Vogiatzis, Engin Tola, 和 Henrik Aanæs. 2014. 大规模多视图立体评估. 在 _Proceedings of the IEEE conference on computer vision and pattern recognition_. 406–413.
  * Jiang 等 (2023) Yingwenqi Jiang, Jiadong Tu, Yuan Liu, Xifeng Gao, Xiaoxiao Long, Wenping Wang, 和 Yuexin Ma. 2023. GaussianShader: 带有反射表面的 shading 函数的 3D 高斯喷溅. _arXiv preprint arXiv:2311.17977_ (2023).
  * Kazhdan 和 Hoppe (2013) Michael Kazhdan 和 Hugues Hoppe. 2013. 屏幕泊松表面重建. _ACM Transactions on Graphics (ToG)_ 32, 3 (2013), 1–13.
  * Kerbl 等 (2023) Bernhard Kerbl, Georgios Kopanas, Thomas Leimkühler, 和 George Drettakis. 2023. 实时辐射场渲染的 3D 高斯喷溅. _ACM Transactions on Graphics_ 42, 4 (2023 年 7 月). <https://repo-sam.inria.fr/fungraph/3d-gaussian-splatting/>
  * Keselman 和 Hebert (2022) Leonid Keselman 和 Martial Hebert. 2022. 具有代数表面的近似可微渲染. 在 _European Conference on Computer Vision_. Springer, 596–614.
  * Keselman 和 Hebert (2023) Leonid Keselman 和 Martial Hebert. 2023. 具有 3D 高斯的可微渲染的灵活技术. _arXiv preprint arXiv:2308.14737_ (2023).
  * Knapitsch 等 (2017) Arno Knapitsch, Jaesik Park, Qian-Yi Zhou, 和 Vladlen Koltun. 2017. Tanks and Temples: 大规模场景重建基准测试. _ACM Transactions on Graphics_ 36, 4 (2017).
  * Kopanas 等 (2021) Georgios Kopanas, Julien Philip, Thomas Leimkühler, 和 George Drettakis. 2021. 基于点的神经渲染与每视图优化. 在 _Computer Graphics Forum_ , Vol. 40. Wiley Online Library, 29–43.
  * Lassner 和 Zollhofer (2021) Christoph Lassner 和 Michael Zollhofer. 2021. Pulsar: 高效的基于球体的神经渲染. 在 _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_. 1440–1449.
  * Li 等 (2023) Zhaoshuo Li, Thomas Müller, Alex Evans, Russell H Taylor, Mathias Unberath, Ming-Yu Liu, 和 Chen-Hsuan Lin. 2023. Neuralangelo: 高保真神经表面重建. 在 _IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_.
  * Liang 等 (2023) Zhihao Liang, Qi Zhang, Ying Feng, Ying Shan, 和 Kui Jia. 2023. GS-IR: 用于逆向渲染的 3D 高斯喷溅. _arXiv preprint arXiv:2311.16473_ (2023).
  * Liu 等 (2020) Lingjie Liu, Jiatao Gu, Kyaw Zaw Lin, Tat-Seng Chua, 和 Christian Theobalt. 2020. 神经稀疏体素场. _NeurIPS_ (2020).
  * Luiten 等 (2024) Jonathon Luiten, Georgios Kopanas, Bastian Leibe, 和 Deva Ramanan. 2024. 动态 3D 高斯: 通过持久动态视图合成进行跟踪. 在 _3DV_.
  * Mescheder 等 (2019) Lars Mescheder, Michael Oechsle, Michael Niemeyer, Sebastian Nowozin, 和 Andreas Geiger. 2019. 占用网络: 在函数空间中学习 3D 重建. 在 _Conference on Computer Vision and Pattern Recognition (CVPR)_.
  * Mildenhall 等 (2020) Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik, Jonathan T. Barron, Ravi Ramamoorthi, 和 Ren Ng. 2020. NeRF: 将场景表示为神经辐射场以进行视图合成. 在 _ECCV_.
  * Mildenhall 等 (2021) Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik, Jonathan T Barron, Ravi Ramamoorthi, 和 Ren Ng. 2021. Nerf: 将场景表示为神经辐射场以进行视图合成. _Commun. ACM_ 65, 1 (2021), 99–106.
  * Müller 等 (2022) Thomas Müller, Alex Evans, Christoph Schied, 和 Alexander Keller. 2022. 具有多分辨率哈希编码的即时神经图形原语. _ACM Trans. Graph._ 41, 4, Article 102 (2022 年 7 月), 15 页.
  * Niemeyer 等 (2020) Michael Niemeyer, Lars Mescheder, Michael Oechsle, 和 Andreas Geiger. 2020. 可微体积渲染: 学习没有 3D 监督的隐式 3D 表示. 在 _Conference on Computer Vision and Pattern Recognition (CVPR)_.
  * Oechsle 等 (2021) Michael Oechsle, Songyou Peng, 和 Andreas Geiger. 2021. UNISURF: 统一神经隐式表面和辐射场以进行多视图重建. 在 _International Conference on Computer Vision (ICCV)_.
  * Park 等 (2019) Jeong Joon Park, Peter Florence, Julian Straub, Richard Newcombe, 和 Steven Lovegrove. 2019. DeepSDF: 学习用于形状表示的连续有符号距离函数. 在 _The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_.
  * Pfister 等 (2000) Hanspeter Pfister, Matthias Zwicker, Jeroen Van Baar, 和 Markus Gross. 2000. Surfels: 作为渲染原语的表面元素. 在 _Proceedings of the 27th annual conference on Computer graphics and interactive techniques_. 335–342.
  * Qian 等 (2023) Shenhan Qian, Tobias Kirschstein, Liam Schoneveld, Davide Davoli, Simon Giebenhain, 和 Matthias Nießner. 2023. GaussianAvatars: 具有绑定的 3D 高斯的真实感头部化身. _arXiv preprint arXiv:2312.02069_ (2023).
  * Reiser 等 (2021) Christian Reiser, Songyou Peng, Yiyi Liao, 和 Andreas Geiger. 2021. KiloNeRF: 用数千个小型 MLP 加速神经辐射场. 在 _International Conference on Computer Vision (ICCV)_.
  * Reiser 等 (2023) Christian Reiser, Rick Szeliski, Dor Verbin, Pratul Srinivasan, Ben Mildenhall, Andreas Geiger, Jon Barron, 和 Peter Hedman. 2023. Merf: 适用于无界场景实时视图合成的内存高效辐射场. _ACM Transactions on Graphics (TOG)_ 42, 4 (2023), 1–12.
  * Rückert 等 (2022) Darius Rückert, Linus Franke, 和 Marc Stamminger. 2022. Adop: 近似可微的一像素点渲染. _ACM Transactions on Graphics (ToG)_ 41, 4 (2022), 1–14.
  * Schönberger 和 Frahm (2016) Johannes Lutz Schönberger 和 Jan-Michael Frahm. 2016. 从运动重访结构. 在 _Conference on Computer Vision and Pattern Recognition (CVPR)_.
  * Schönberger 等 (2016) Johannes Lutz Schönberger, Enliang Zheng, Marc Pollefeys, 和 Jan-Michael Frahm. 2016. 用于非结构化多视图立体的逐像素视图选择. 在 _European Conference on Computer Vision (ECCV)_.
  * Schöps 等 (2019) Thomas Schöps, Torsten Sattler, 和 Marc Pollefeys. 2019. Surfelmeshing: 在线基于 surfel 的网格重建. _IEEE transactions on pattern analysis and machine intelligence_ 42, 10 (2019), 2494–2507.
  * Shi 等 (2023) Yahao Shi, Yanmin Wu, Chenming Wu, Xing Liu, Chen Zhao, Haocheng Feng, Jingtuo Liu, Liangjun Zhang, Jian Zhang, Bin Zhou, Errui Ding, 和 Jingdong Wang. 2023. GIR: 用于可重光照场景因子的 3D 高斯逆向渲染. _Arxiv_ (2023). arXiv:2312.05133
  * Sigg 等 (2006) Christian Sigg, Tim Weyrich, Mario Botsch, 和 Markus H Gross. 2006. 基于 GPU 的二次曲面光线投射. 在 _PBG@ SIGGRAPH_. 59–65.
  * Sun 等 (2022a) Cheng Sun, Min Sun, 和 Hwann-Tzong Chen. 2022a. 直接体素网格优化: 辐射场重建的超快收敛. 在 _CVPR_.
  * Sun 等 (2022b) Cheng Sun, Min Sun, 和 Hwann-Tzong Chen. 2022b. 辐射场重建的改进直接体素网格优化. _arxiv cs.GR 2206.05085_ (2022).
  * Wang 等 (2021) Peng Wang, Lingjie Liu, Yuan Liu, Christian Theobalt, Taku Komura, 和 Wenping Wang. 2021. NeuS: 通过体积渲染学习神经隐式表面以进行多视图重建. _Advances in Neural Information Processing Systems_ 34 (2021), 27171–27183.
  * Wang 等 (2023) Yiming Wang, Qin Han, Marc Habermann, Kostas Daniilidis, Christian Theobalt, 和 Lingjie Liu. 2023. NeuS2: 快速学习神经隐式表面以进行多视图重建. 在 _Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)_.
  * Weyrich 等 (2007) Tim Weyrich, Simon Heinzle, Timo Aila, Daniel B Fasnacht, Stephan Oetiker, Mario Botsch, Cyril Flaig, Simon Mall, Kaspar Rohrer, Norbert Felber, 等. 2007. 一种用于表面喷溅的硬件架构. _ACM Transactions on Graphics (TOG)_ 26, 3 (2007), 90–es.
  * Whelan 等 (2016) Thomas Whelan, Renato F Salas-Moreno, Ben Glocker, Andrew J Davison, 和 Stefan Leutenegger. 2016. ElasticFusion: 实时稠密 SLAM 和光源估计. _The International Journal of Robotics Research_ 35, 14 (2016), 1697–1716.
  * Wiles 等 (2020) Olivia Wiles, Georgia Gkioxari, Richard Szeliski, 和 Justin Johnson. 2020. SynSin: 从单幅图像进行端到端视图合成. 在 _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_.
  * Xie 等 (2023) Tianyi Xie, Zeshun Zong, Yuxing Qiu, Xuan Li, Yutao Feng, Yin Yang, 和 Chenfanfu Jiang. 2023. PhysGaussian: 用于生成动力学的物理集成 3D 高斯. _arXiv preprint arXiv:2311.12198_ (2023).
  * Yan 等 (2023) Yunzhi Yan, Haotong Lin, Chenxu Zhou, Weijie Wang, Haiyang Sun, Kun Zhan, Xianpeng Lang, Xiaowei Zhou, 和 Sida Peng. 2023. 用于动态城市场景建模的街道高斯. (2023).
  * Yao 等 (2018) Yao Yao, Zixin Luo, Shiwei Li, Tian Fang, 和 Long Quan. 2018. MVSNet: 用于非结构化多视图立体的深度推断. _European Conference on Computer Vision (ECCV)_ (2018).
  * Yariv 等 (2021) Lior Yariv, Jiatao Gu, Yoni Kasten, 和 Yaron Lipman. 2021. 神经隐式表面的体积渲染. _Advances in Neural Information Processing Systems_ 34 (2021), 4805–4815.
  * Yariv 等 (2023) Lior Yariv, Peter Hedman, Christian Reiser, Dor Verbin, Pratul P. Srinivasan, Richard Szeliski, Jonathan T. Barron, 和 Ben Mildenhall. 2023. BakedSDF: 用于实时视图合成的神经 SDF 网格化. _arXiv_ (2023).
  * Yariv 等 (2020) Lior Yariv, Yoni Kasten, Dror Moran, Meirav Galun, Matan Atzmon, Basri Ronen, 和 Yaron Lipman. 2020. 通过解耦几何和外观进行的多视图神经表面重建. _Advances in Neural Information Processing Systems_ 33 (2020).
  * Yifan 等 (2019) Wang Yifan, Felice Serena, Shihao Wu, Cengiz Öztireli, 和 Olga Sorkine-Hornung. 2019. 可微表面喷溅用于基于点的几何处理. _ACM Transactions on Graphics (TOG)_ 38, 6 (2019), 1–14.
  * Yu 等 (2021) Alex Yu, Ruilong Li, Matthew Tancik, Hao Li, Ren Ng, 和 Angjoo Kanazawa. 2021. PlenOctrees: 实时渲染神经辐射场. 在 _ICCV_.
  * Yu 等 (2022a) Zehao Yu, Anpei Chen, Bozidar Antic, Songyou Peng, Apratim Bhattacharyya, Michael Niemeyer, Siyu Tang, Torsten Sattler, 和 Andreas Geiger. 2022a. SDFStudio: 一种统一的表面重建框架. <https://github.com/autonomousvision/sdfstudio>
  * Yu 等 (2024) Zehao Yu, Anpei Chen, Binbin Huang, Torsten Sattler, 和 Andreas Geiger. 2024. Mip-Splatting: 无别名的 3D 高斯喷溅. _Conference on Computer Vision and Pattern Recognition (CVPR)_ (2024).
  * Yu 和 Gao (2020) Zehao Yu 和 Shenghua Gao. 2020. Fast-MVSNet: 具有学习传播和高斯牛顿优化的稀疏到密集多视图立体. 在 _Conference on Computer Vision and Pattern Recognition (CVPR)_.
  * Yu 等 (2022b) Zehao Yu, Songyou Peng, Michael Niemeyer, Torsten Sattler, 和 Andreas Geiger. 2022b. MonoSDF: 探索单目几何线索以进行神经隐式表面重建. _Advances in Neural Information Processing Systems (NeurIPS)_ (2022).
  * Zhang 等 (2020) Kai Zhang, Gernot Riegler, Noah Snavely, 和 Vladlen Koltun. 2020. NeRF++: 分析和改进神经辐射场. _arXiv:2010.07492_ (2020).
  * Zhou 等 (2018) Qian-Yi Zhou, Jaesik Park, 和 Vladlen Koltun. 2018. Open3D: 现代 3D 数据处理库. _arXiv:1801.09847_ (2018).
  * Zielonka 等 (2023) Wojciech Zielonka, Timur Bagautdinov, Shunsuke Saito, Michael Zollhöfer, Justus Thies, 和 Javier Romero. 2023. 可驱动的 3D 高斯化身. (2023). arXiv:2311.08581 [cs.CV]
  * Zwicker 等 (2001a) Matthias Zwicker, Hanspeter Pfister, Jeroen Van Baar, 和 Markus Gross. 2001a. EWA 体积喷溅. 在 _Proceedings Visualization, 2001. VIS’01._ IEEE, 29–538.
  * Zwicker 等 (2001b) Matthias Zwicker, Hanspeter Pfister, Jeroen Van Baar, 和 Markus Gross. 2001b. 表面喷溅. 在 _Proceedings of the 28th annual conference on Computer graphics and interactive techniques_. 371–378.
  * Zwicker 等 (2004) Matthias Zwicker, Jussi Rasanen, Mario Botsch, Carsten Dachsbacher, 和 Mark Pauly. 2004. 透视精确喷溅. 在 _Proceedings-Graphics Interface_. 247–254.### 翻译
## 附录 A 深度畸变的详细信息### 翻译
虽然 Barron 等人 (Barron et al., [2022b](https://arxiv.org/html/2403.17888v3#bib.bib5)) 在光线上的样本中计算失真损失，但我们操作的是高斯基元，其中相交的深度可能是无序的。为此，我们采用了 ℒ2\mathcal{L}_{2} 损失，并将相交的深度 z𝑧z 转换到 NDC 空间，以减少远离的高斯基元的权重，m=NDC⁢(z)𝑚NDC𝑧m=\text{NDC}(z)，其近平面和远平面经验上分别设定为 0.2 和 1000。我们基于 (Sun et al., [2022b](https://arxiv.org/html/2403.17888v3#bib.bib51)) 实现了我们的深度失真损失，也得益于基于瓦片的渲染。在这里，我们展示了嵌套算法可以在单次前向传递中实现：### 翻译
\[
\mathcal{L}=\sum_{i=0}^{N-1}\sum_{j=0}^{i-1}\omega_{i}\omega_{j}(m_{i}-m_{j})^{2}=\sum_{i=0}^{N-1}\omega_{i}\left(m_{i}^{2}\sum_{j=0}^{i-1}\omega_{j}+\sum_{j=0}^{i-1}\omega_{j}m_{j}^{2}-2m_{i}\sum_{j=0}^{i-1}\omega_{j}m_{j}\right)=\sum_{i=0}^{N-1}\omega_{i}\left(m_{i}^{2}A_{i-1}+D^{2}_{i-1}-2m_{i}D_{i-1}\right),
\]
其中 
\[
A_{i}=\sum_{j=0}^{i}\omega_{j},
\]
\[
D_{i}=\sum_{j=0}^{i}\omega_{j}m_{j},
\]
和 
\[
D^{2}_{i}=\sum_{j=0}^{i}\omega_{j}m_{j}^{2}。
\]### 翻译
具体来说，我们设定
e_{i} = (m_{i}^{2}A_{i-1} + D^{2}_{i-1} - 2m_{i}D_{i-1}) 以便于失真损失可以被“呈现”为
ℒ_{i} = ∑_{j=0}^{i}ω_{j}e_{j}。在这里，ℒ_{i} 计算到第 i 个高斯的深度失真。在从前到后推进高斯的过程中，我们同时累积 A_{i}、D_{i} 和 D^{2}_{i}，为下一个失真计算 ℒ_{i+1} 做准备。同样，深度失真的梯度可以反向传播到原始数据中。与隐式方法不同，其中 m 是预定义的采样深度且不可微分，我们还通过交点 m 反向传播梯度，鼓励高斯直接紧密靠在一起。### 翻译
![Refer to caption](x5.png) 图 7: 由 2D Gaussian 组成的平面可视化。3DGS 中采用的仿射近似 (Zwicker et al., [2001b](https://arxiv.org/html/2403.17888v3#bib.bib73)) 导致透视失真和深度不准确，违反了正常一致性。### 翻译
## 附录 B 深度计算### 翻译
#### 平均深度:### 翻译
我们在网格处理过程中使用了两种可选的深度计算方法。均值（期望）深度是通过对相交深度进行加权计算得出的：### 翻译
(18) |  | zmean=∑iωi⁢zi/(∑iωi+ϵ)  
其中  
ωi=Tiαi𝒢^i(𝐮(𝐱)  
是第 i 个高斯的权重贡献，  
Ti=∏j=1i−1(1−αj⁢𝒢^j⁢(𝐮⁢(𝐱)))  
衡量其可见性。对深度进行归一化是重要的，  
A=∑iωi𝐴  
以确保一个 2D 高斯可以在深度可视化中呈现为一个平面 2D 磁盘。### 翻译
#### 中位深度:### 翻译
我们将中位深度计算为最大的“可见”深度，考虑 Ti=0.5 作为表面和自由空间的支点：### 翻译
(19) |  | zmedian=max⁡{zi⁢|Ti>⁢0.5}.subscript𝑧mediansubscript𝑧𝑖ketsubscript𝑇𝑖0.5z_{\text{median}}=\max\\{z_{i}|T_{i}>0.5\\}.italic_z start_POSTSUBSCRIPT median end_POSTSUBSCRIPT = roman_max { italic_z start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT | italic_T start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT > 0.5 } . |   
---|---|---|---  
  
我们发现我们的中位深度计算对于 (Luiten et al., [2024](https://arxiv.org/html/2403.17888v3#bib.bib32)) 更为稳健。当光线的累积 alpha 没有达到 0.5 时，Luiten et al. 设置了默认值为 15，而我们的计算选择最后一个高斯，这更加准确且适合用于训练。### 翻译
(a) 真实情况 ![参见说明](x6.png) ![参见说明](x7.png) (b) MipNeRF360 (Barron 等人, [2022b](https://arxiv.org/html/2403.17888v3#bib.bib5)), SSIM=0.813### 翻译
(c) 3DGS，从深度梯度获取法线！[参见说明](x8.png) ![参见说明](x9.png) (d) 3DGS (Kerbl 等人，[2023](https://arxiv.org/html/2403.17888v3#bib.bib23))，SSIM=0.834 (e) 我们的模型 (2DGS)，从深度梯度获取法线！[参见说明](x10.png) ![参见说明](x11.png) (f) 我们的模型 (2DGS)，SSIM=0.845### 翻译
图 8: 我们可视化了由 MipNeRF360 (Barron et al., [2022b](https://arxiv.org/html/2403.17888v3#bib.bib5))、3DGS (Kerbl et al., [2023](https://arxiv.org/html/2403.17888v3#bib.bib23)) 和我们的方法生成的深度图。3DGS (d) 和 2DGS (f) 的深度图是使用公式 [18](https://arxiv.org/html/2403.17888v3#A2.E18 "在平均深度中: ‣ 附录 B 深度计算 ‣ 适用于几何准确辐射场的 2D 高斯溅射") 渲染的，并按照 MipNeRF360 进行可视化。为了突出表面光滑度，我们进一步可视化了使用公式 [15](https://arxiv.org/html/2403.17888v3#S5.E15 "在法线一致性 ‣ 5. 训练 ‣ 适用于几何准确辐射场的 2D 高斯溅射") 从深度梯度估计的法线，分别针对 3DGS (c) 和我们的方法 (e)。虽然 MipNeRF360 能够生成看似光滑的深度图，但其采样过程可能导致细节结构的丢失。3DGS 和 2DGS 在建模细薄结构方面表现卓越；然而，如 (c) 和 (e) 所示，3DGS 的深度图存在显著噪声。相比之下，我们的方法生成的采样深度点与渲染法线图 (参见图 [1](https://arxiv.org/html/2403.17888v3#S0.F1 "图 1 ‣ 适用于几何准确辐射场的 2D 高斯溅射")b) 中的法线一致，从而增强了网格化过程中的深度融合。### 翻译
## 附录 C 额外基线### 翻译
在本节中，我们展示了额外的基线，以逐步分析我们设计选择的影响，如表 [6](https://arxiv.org/html/2403.17888v3#A3.T6 "Table 6 ‣ Appendix C Additional baselines ‣ 2D Gaussian Splatting for Geometrically Accurate Radiance Fields") 中所总结。此外，我们将我们的网格生成方法整合到与这些基线的比较中，以进行全面分析。SuGaR 在粗略阶段利用 SPSR（屏幕泊松表面重建）从深度点提取网格，然后通过网格渲染器进行细化。为了评估这种网格生成策略的效果，我们用深度图替代了 SPSR，采用 TSDF，随后进行相同的细化阶段。然而，我们发现从其平坦的高斯交集生成的深度图稀疏且不连续。因此，采用具有不连续深度图的 TSDF 会产生较差的结果。对于 3DGS，我们利用 SPSR 进行网格生成。由于 3D 高斯缺乏表面法线，我们将其法线视为可训练参数（Gao et al., [2023](https://arxiv.org/html/2403.17888v3#bib.bib15)；Liang et al., [2023](https://arxiv.org/html/2403.17888v3#bib.bib30)），从深度图中提取，并采用法线一致性正则化。然后，我们利用所有中心点进行 SPSR，相较于使用 TSDF 的结果，整体完成度指标得到了改善。### 翻译
最后，我们在我们的 2DGS 上进行消融实验。值得注意的是，2DGS 通过迭代整合 TSDF、透视校正光栅化和中位深度等组件，显示出增强的性能。对于仿射近似基线，我们利用 3DGS 的光栅化方法，通过将一个 3D 高斯的尺度配置为 1⁢e−61 (𝑒^{-6})。虽然仿射近似已经产生了较好的结果，但整合提议的光线散点求交方案在透视投影下生成了更准确的深度图，如图 [7](https://arxiv.org/html/2403.17888v3#A1.F7 "Figure 7 ‣ Appendix A Details of Depth Distortion ‣ 2D Gaussian Splatting for Geometrically Accurate Radiance Fields") 所示，从而增强了深度融合性能。### 翻译
表 6: DTU 数据集上的额外基准。所有模型均经过 30k𝑘kitalic_k 次迭代训练。### 翻译
| 准确率 ↓↓\downarrow↓ | 完成度 ↓↓\downarrow↓ | 平均值 ↓↓\downarrow↓  
---|---|---|---  
SuGaR | 1.48 | 1.17 | 1.33  
SuGaR + TSDF | 2.47 | 1.90 | 2.18  
3DGS + SPSR (中心) | 2.05 | 1.25 | 1.65  
3DGS + TSDF (均值) | 1.93 | 1.99 | 1.96  
2DGS + SPSR (中心) | 1.25 | 0.89 | 1.07  
2DGS (仿射近似) + TSDF (均值) | 0.96 | 1.20 | 1.08  
2DGS (我们的光栅化器) + TSDF (均值) | 0.79 | 0.98 | 0.88  
2DGS (我们的光栅化器) + TSDF (中位数) | 0.78 | 0.83 | 0.80  
  
## 附录 D 额外结果### 翻译
我们的 2D 高斯点洇方法在没有正则化的情况下也能达到可比较的性能，如表 [7](https://arxiv.org/html/2403.17888v3#A4.T7 "Table 7 ‣ Appendix D Additional Results ‣ 2D Gaussian Splatting for Geometrically Accurate Radiance Fields") 所示。我们在表 [9](https://arxiv.org/html/2403.17888v3#A4.T9 "Table 9 ‣ Appendix D Additional Results ‣ 2D Gaussian Splatting for Geometrically Accurate Radiance Fields") 中列出了 MipNeRF360 数据集 (Barron et al., [2022b](https://arxiv.org/html/2403.17888v3#bib.bib5)) 的每场景指标的详细细分。此外，我们在图 [8](https://arxiv.org/html/2403.17888v3#A2.F8 "Figure 8 ‣ Median depth: ‣ Appendix B Depth calculations ‣ 2D Gaussian Splatting for Geometrically Accurate Radiance Fields") 中提供了我们渲染的深度图与 3DGS 和 MipNeRF360 的比较。### 翻译
表7: 合成 NeRF 数据集的 PSNR 分数。我们的模型在不使用正则化的情况下取得了可比的性能。### 翻译
| Mic | Chair | Ship | Materials | Lego | Drums | Ficus | Hotdog | Mean  
---|---|---|---|---|---|---|---|---|---  
Plenoxels | 33.26 | 33.98 | 29.62 | 29.14 | 34.10 | 25.35 | 31.83 | 36.81 | 31.76  
INGP-Base | 36.22 | 35.00 | 31.10 | 29.78 | 36.39 | 26.02 | 33.51 | 37.40 | 33.18  
Mip-NeRF | 36.51 | 35.14 | 30.41 | 30.71 | 35.70 | 25.48 | 33.29 | 37.48 | 33.09  
3DGS | 35.36 | 35.83 | 30.80 | 30.00 | 35.78 | 26.15 | 34.87 | 37.72 | 33.32  
Ours | 35.09 | 35.05 | 30.60 | 29.74 | 35.10 | 26.05 | 35.57 | 37.36 | 33.07  
  
表 8. TnT 数据集的 PSNR 分数。### 翻译
| Barn | Caterpillar | Courthouse | Ignatius | Meetingroom | Truck | Mean  
---|---|---|---|---|---|---|---  
SuGaR | 28.63 | 23.27 | 23.33 | 20.72 | 25.47 | 24.40 | 24.16  
3DGS | 27.99 | 24.82 | 23.33 | 23.95 | 26.89 | 25.01 | 25.33  
Ours | 28.79 | 24.23 | 23.51 | 23.82 | 26.15 | 26.85 | 25.56  
  
表 9: PSNR ↑↑\uparrow↑, SSIM ↑↑\uparrow↑, LIPPS ↓↓\downarrow↓ 分数对于 MipNeRF360 数据集。### 翻译
| 自行车 | 花卉 | 花园 | 树桩 | 树丘 | 房间 | 计数器 | 厨房 | 盆栽 | 平均  
---|---|---|---|---|---|---|---|---|---|---  
SugaR | 23.34 | 19.54 | 25.40 | 25.07 | 21.30 | 29.97 | 27.56 | 29.41 | 30.77 | 25.82  
3DGS | 25.24 | 21.52 | 27.41 | 26.55 | 22.49 | 30.63 | 28.70 | 30.32 | 31.98 | 27.20  
Ours | 24.87 | 21.15 | 26.95 | 26.47 | 22.27 | 31.06 | 28.55 | 30.50 | 31.52 | 27.03  
SuGaR | 0.634 | 0.499 | 0.762 | 0.705 | 0.546 | 0.904 | 0.885 | 0.902 | 0.933 | 0.752  
3DGS | 0.771 | 0.605 | 0.868 | 0.775 | 0.638 | 0.914 | 0.905 | 0.922 | 0.938 | 0.815  
Ours | 0.752 | 0.588 | 0.852 | 0.765 | 0.627 | 0.912 | 0.900 | 0.919 | 0.933 | 0.805  
SuGaR | 0.354 | 0.407 | 0.240 | 0.325 | 0.452 | 0.259 | 0.244 | 0.178 | 0.220 | 0.298  
3DGS | 0.205 | 0.336 | 0.103 | 0.210 | 0.317 | 0.220 | 0.204 | 0.129 | 0.205 | 0.214  
Ours | 0.218 | 0.346 | 0.115 | 0.222 | 0.329 | 0.223 | 0.208 | 0.133 | 0.214 | 0.223  

![Refer to caption](x12.png) 图 9: 使用我们的 2DGS 和 3DGS (Kerbl et al., [2023](https://arxiv.org/html/2403.17888v3#bib.bib23)) 进行的表面重建比较。通过将 TSDF 应用于深度图提取网格。 ![Refer to caption](x13.png) 图 10: Tanks and Temples 数据集的定性研究 (Knapitsch et al., [2017](https://arxiv.org/html/2403.17888v3#bib.bib26))。 ![Refer to caption](x14.png) 图 11: 从重建的 2D 高斯圆盘中获得的外观渲染结果，包括 DTU、TnT 和 Mip-NeRF360 数据集。 ![Refer to caption](x15.png) 图 12: 限制的示意图：我们的 2DGS 在准确重建半透明表面方面存在困难，例如示例 (A) 中显示的玻璃。此外，我们的方法在高光强度区域往往会产生孔，如 (B) 中所示。### 翻译
您被训练于截至 2023 年 10 月的数据。### 翻译
你所接受的训练数据截止到2023年10月。